////////////////////////////////////////////////////////////////////////////
// MESI-cache actions definitions
////////////////////////////////////////////////////////////////////////////
// ACTIONS

action(a_issueFetchToMemory, "a", desc="fetch data from memory") {
    peek(L1RequestL2Network_in, RequestMsg) {
        enqueue(DirRequestL2Network_out, RequestMsg, l2_request_latency) {
            DPRINTF(RubySlicc, "a_issueFetchToMemory::addr=%x, Requestor=%d\n"
                             , address, machineID);
            out_msg.addr := address;
            out_msg.Type := CoherenceRequestType:GETS;
            out_msg.Requestor := machineID;
            out_msg.Destination.add(mapAddressToMachine(address, MachineType:Directory));
            out_msg.MessageSize := MessageSizeType:Control;
            out_msg.AccessMode := in_msg.AccessMode;
            out_msg.Prefetch := in_msg.Prefetch;
        }
    }
}

action(b_forwardRequestToExclusive, "b", desc="Forward request to the exclusive L1") {
    peek(L1RequestL2Network_in, RequestMsg) {
        if(is_invalid(getDirEntry(address))) {
            // allocate directory entry
            assert(!SF.isTagPresent(address));
            // SF must be available
            assert(SF.cacheAvail(address));
            SF.allocate(address, new DirEntry);
            addSharer(address, in_msg.Requestor, getDirEntry(address));
        }
        DirEntry dir_entry := getDirEntry(address);
        DPRINTF(RubySlicc, "b_forwardRequestToExclusive::addr=%x, requestor=%d, dir count=%d, cache valid=%d\n"
                         , address, in_msg.Requestor, dir_entry.Sharers.count(), is_valid(cache_entry));

        assert(dir_entry.Sharers.count() == 1);
        if (dir_entry.Sharers.isElement(in_msg.Requestor)) {
            // this is the situation that cache is avail
            assert(is_valid(cache_entry));
            // but directory is invalid
            enqueue(responseL2Network_out, ResponseMsg, l2_response_latency) {
                out_msg.addr := address;
                if (in_msg.Type == CoherenceRequestType:GETS ||
                    in_msg.Type == CoherenceRequestType:GET_INSTR) {
                    out_msg.Type := CoherenceResponseType:DATA_EXCLUSIVE;
                } else {
                    assert(in_msg.Type == CoherenceRequestType:GETX);
                    out_msg.Type := CoherenceResponseType:DATA;
                }
                out_msg.Sender := machineID;
                out_msg.Destination.add(in_msg.Requestor);
                out_msg.DataBlk := cache_entry.DataBlk;
                out_msg.MessageSize := MessageSizeType:Response_Data;
                out_msg.AckCount := 0 - dir_entry.Sharers.count();
                out_msg.AckCount := out_msg.AckCount + 1;
            }
        } else {
            enqueue(L1RequestL2Network_out, RequestMsg, to_l1_latency) {
                // formal
                out_msg.addr := address;
                out_msg.Type := in_msg.Type;
                out_msg.Requestor := in_msg.Requestor;
                out_msg.Destination.add(dir_entry.Exclusive);
                out_msg.MessageSize := MessageSizeType:Request_Control;
            }
        }
    }
}

action(c_exclusiveReplacement, "c", desc="Send data to memory") {
    assert(is_valid(cache_entry));
    DirEntry dir_entry := getDirEntry(address);
    bool isCacheEvict := false;
    if(triggerInPort.isReady(clockEdge())) {
        peek(triggerInPort, RequestMsg) {
            isCacheEvict := (in_msg.Type == CoherenceRequestType:EVC);
        }
    }
    DPRINTF(RubySlicc, "c_exclusiveReplacement::addr=%x, dir valid=%d, dirty=%d, isCacheEvict=%d, data=%s\n"
                     , address, is_valid(dir_entry), cache_entry.Dirty, isCacheEvict, cache_entry.DataBlk);
    enqueue(responseL2Network_out, ResponseMsg, l2_response_latency) {
        if (is_invalid(dir_entry)) {
            // the upstream does not have the block, so the 
            // eviction should write back to mem
            if (!isDirty(cache_entry) && isCacheEvict) {
                out_msg.addr := address;
                out_msg.Type := CoherenceResponseType:ACK;
                out_msg.Sender := machineID;
                out_msg.Destination.add(mapAddressToMachine(address, MachineType:Directory));
                out_msg.MessageSize := MessageSizeType:Response_Control;
            } else {
                out_msg.addr := address;
                out_msg.Type := CoherenceResponseType:MEMORY_DATA;
                out_msg.Sender := machineID;
                out_msg.Destination.add(mapAddressToMachine(address, MachineType:Directory));
                out_msg.DataBlk := cache_entry.DataBlk;
                out_msg.Dirty := cache_entry.Dirty;
                out_msg.MessageSize := MessageSizeType:Response_Data;
            }
        } else {
            out_msg.addr := address;
            out_msg.Type := CoherenceResponseType:ACK_No_to_Mem;
            out_msg.Sender := machineID;
            out_msg.Destination.add(mapAddressToMachine(address, MachineType:Directory));
            out_msg.MessageSize := MessageSizeType:Response_Control;
        }
    }
}

action(c_exclusiveCleanReplacement, "cc", desc="Send ack to memory for clean replacement") {
    enqueue(responseL2Network_out, ResponseMsg, l2_response_latency) {
        DPRINTF(RubySlicc, "c_exclusiveCleanReplacement::addr=%x\n", address);
        out_msg.addr := address;
        out_msg.Type := CoherenceResponseType:ACK;
        out_msg.Sender := machineID;
        out_msg.Destination.add(mapAddressToMachine(address, MachineType:Directory));
        out_msg.MessageSize := MessageSizeType:Response_Control;
    }
}

action(ct_exclusiveReplacementFromTBE, "ct", desc="Send data to memory") {
    enqueue(responseL2Network_out, ResponseMsg, l2_response_latency) {
        assert(is_valid(tbe));
        out_msg.addr := address;
        out_msg.Type := CoherenceResponseType:MEMORY_DATA;
        out_msg.Sender := machineID;
        out_msg.Destination.add(mapAddressToMachine(address, MachineType:Directory));
        out_msg.DataBlk := tbe.DataBlk;
        out_msg.Dirty := tbe.Dirty;
        out_msg.MessageSize := MessageSizeType:Response_Data;
    }
}

action(d_sendDataToRequestor, "d", desc="Send data from cache to reqeustor") {
    peek(L1RequestL2Network_in, RequestMsg) {
        DPRINTF(RubySlicc, "d_sendDataToRequestor::addr=%x, Type=%d\n"
                         , address, in_msg.Type);
        if (is_valid(cache_entry)) {
            enqueue(responseL2Network_out, ResponseMsg, l2_response_latency) {
                out_msg.addr := address;
                out_msg.Type := CoherenceResponseType:DATA;
                out_msg.Sender := machineID;
                out_msg.Destination.add(in_msg.Requestor);
                out_msg.DataBlk := cache_entry.DataBlk;
                out_msg.MessageSize := MessageSizeType:Response_Data;
                if(is_invalid(getDirEntry(address))) {
                    // allocate directory entry
                    assert(!SF.isTagPresent(address));
                    // SF must be available
                    assert(SF.cacheAvail(address));
                    SF.allocate(address, new DirEntry);
                    addSharer(address, in_msg.Requestor, getDirEntry(address));
                }

                DirEntry dir_entry := getDirEntry(address);
                DPRINTF(RubySlicc, "d_sendDataToRequestor::addr=%x, Sharers=%s\n"
                                 , address, dir_entry.Sharers);
                out_msg.AckCount := 0 - dir_entry.Sharers.count();
                if (dir_entry.Sharers.isElement(in_msg.Requestor)) {
                    out_msg.AckCount := out_msg.AckCount + 1;
                }
            }
        } else {
            assert(getState(tbe, cache_entry, address) == State:SS);
            assert(is_valid((getDirEntry(address))));
            assert(in_msg.Type == CoherenceRequestType:GETX);
            DirEntry dir_entry := getDirEntry(address);
            enqueue(L1RequestL2Network_out, RequestMsg, 1) {
                // if cache is invalid, so that we need to send
                // request to upsteam
                out_msg.addr := address;
                out_msg.Type := CoherenceRequestType:GETX;
                out_msg.Requestor := in_msg.Requestor;
                out_msg.Destination.add(dir_entry.Sharers.smallestElement());
                out_msg.MessageSize := MessageSizeType:Request_Control;

                // assert(dir_entry.Sharers.count() > 1);
                // out_msg.AckCount := 0 - dir_entry.Sharers.count();
                // if (dir_entry.Sharers.isElement(in_msg.Requestor)) {
                //     out_msg.AckCount := out_msg.AckCount + 1;
                // }
            }
            enqueue(responseL2Network_out, ResponseMsg, 1) {
                DPRINTF(RubySlicc, "d_sendDataToRequestor::send wait_ack to requestor::addr=%x, requestor=%d, dir_sharers=%d\n"
                                    , address, in_msg.Requestor, dir_entry.Sharers);
                assert(!dir_entry.Sharers.isElement(in_msg.Requestor));
                out_msg.addr := address;
                out_msg.Type := CoherenceResponseType:ACK;
                out_msg.Sender := machineID;
                out_msg.Destination.add(in_msg.Requestor);
                out_msg.MessageSize := MessageSizeType:Response_Control;
                // upgrader doesn't get ack from itself, hence the + 1
                out_msg.AckCount := 0 - dir_entry.Sharers.count();
            }
        }
    }
}

action(dd_sendExclusiveDataToRequestor, "dd", desc="Send data from cache to reqeustor") {
    peek(L1RequestL2Network_in, RequestMsg) {
        enqueue(responseL2Network_out, ResponseMsg, l2_response_latency) {
            assert(is_valid(cache_entry));
            DPRINTF(RubySlicc, "dd_sendExclusiveDataToRequestor::addr=%x, dirty=%d, data=%s\n"
                             , address, cache_entry.Dirty, cache_entry.DataBlk);
            out_msg.addr := address;
            out_msg.Type := CoherenceResponseType:DATA_EXCLUSIVE;
            out_msg.Sender := machineID;
            out_msg.Destination.add(in_msg.Requestor);
            out_msg.DataBlk := cache_entry.DataBlk;
            out_msg.Dirty := cache_entry.Dirty;
            out_msg.MessageSize := MessageSizeType:Response_Data;
            if(is_invalid(getDirEntry(address))) {
                // allocate directory entry
                assert(!SF.isTagPresent(address));
                SF.allocate(address, new DirEntry);
                addSharer(address, in_msg.Requestor, getDirEntry(address));
            }

            DirEntry dir_entry := getDirEntry(address);
            out_msg.AckCount := 0 - dir_entry.Sharers.count();
            if (dir_entry.Sharers.isElement(in_msg.Requestor)) {
                out_msg.AckCount := out_msg.AckCount + 1;
            }
        }
    }
}

action(ds_sendSharedDataToRequestor, "ds", desc="Send data from cache to reqeustor") {
    peek(L1RequestL2Network_in, RequestMsg) {
        DirEntry dir_entry := getDirEntry(address);
        assert(is_valid(tbe));
        DPRINTF(RubySlicc, "ds_sendSharedDataToRequestor::addr=%x, cache valid=%d, dir valid=%d, waitDataNum=%d\n"
                         , address, is_valid(cache_entry), is_valid(dir_entry), tbe.waitDataNum);
        if (is_valid(cache_entry)) {
            enqueue(responseL2Network_out, ResponseMsg, l2_response_latency) {
                out_msg.addr := address;
                out_msg.Type := CoherenceResponseType:DATA;
                out_msg.Sender := machineID;
                out_msg.Destination.add(in_msg.Requestor);
                out_msg.DataBlk := cache_entry.DataBlk;
                out_msg.Dirty := cache_entry.Dirty;
                out_msg.MessageSize := MessageSizeType:Response_Data;
                out_msg.AckCount := 0;
            }
            DPRINTF(RubySlicc, "ds_sendSharedDataToRequestor::has cache line, so deallocate tbe\n");
            TBEs.deallocate(address);
            unset_tbe();
        } else {
            tbe.waitDataNum := tbe.waitDataNum + 1;
            enqueue(L1RequestL2Network_out, RequestMsg, to_l1_latency) {
                assert(is_valid(dir_entry));
                //assert(dir_entry.Sharers.count() > 1);
                out_msg.addr := address;
                out_msg.Type := in_msg.Type;
                out_msg.Requestor := in_msg.Requestor;
                out_msg.Destination.add(dir_entry.Sharers.smallestElement());
                out_msg.MessageSize := MessageSizeType:Request_Control;
            }
        }
    }
}

action(e_sendDataToGetSRequestors, "e", desc="Send data from cache to all GetS IDs") {
    assert(is_valid(tbe));
    assert(tbe.L1_GetS_IDs.count() > 0);
    enqueue(responseL2Network_out, ResponseMsg, to_l1_latency) {
        DirEntry dir_entry := getDirEntry(address);
        assert(is_valid(dir_entry));
        out_msg.addr := address;
        out_msg.Type := CoherenceResponseType:DATA;
        out_msg.Sender := machineID;
        out_msg.Destination := tbe.L1_GetS_IDs;  // internal nodes
        out_msg.DataBlk := tbe.DataBlk;
        out_msg.MessageSize := MessageSizeType:Response_Data;
        DPRINTF(RubySlicc, "e_sendDataToGetSRequestors::addr=%x, Destination: %s, DataBlock: %s\n"
                         , out_msg.addr, out_msg.Destination, out_msg.DataBlk);
    }
}

action(ex_sendExclusiveDataToGetSRequestors, "ex", desc="Send data from cache to all GetS IDs") {
    assert(is_valid(tbe));
    assert(tbe.L1_GetS_IDs.count() == 1);
    enqueue(responseL2Network_out, ResponseMsg, to_l1_latency) {
        DirEntry dir_entry := getDirEntry(address);
        assert(is_valid(dir_entry));
        out_msg.addr := address;
        out_msg.Type := CoherenceResponseType:DATA_EXCLUSIVE;
        out_msg.Sender := machineID;
        out_msg.Destination := tbe.L1_GetS_IDs;  // internal nodes
        out_msg.DataBlk := tbe.DataBlk;
        out_msg.MessageSize := MessageSizeType:Response_Data;
        DPRINTF(RubySlicc, "ex_sendExclusiveDataToGetSRequestors::addr=%x, Destination: %s, DataBlock: %s\n"
                         , out_msg.addr, out_msg.Destination, out_msg.DataBlk);
    }
}

action(ee_sendDataToGetXRequestor, "ee", desc="Send data from cache to GetX ID") {
    enqueue(responseL2Network_out, ResponseMsg, to_l1_latency) {
        assert(is_valid(tbe));
        DirEntry dir_entry := getDirEntry(address);
        assert(is_valid(dir_entry));
        out_msg.addr := address;
        out_msg.Type := CoherenceResponseType:DATA;
        out_msg.Sender := machineID;
        out_msg.Destination.add(tbe.L1_GetX_ID);
        out_msg.DataBlk := tbe.DataBlk;
        DPRINTF(RubySlicc, "ee_sendDataToGetXRequestor::addr=%x, Destination: %s, DataBlock: %s\n"
                         , out_msg.addr, out_msg.Destination, out_msg.DataBlk);
        out_msg.MessageSize := MessageSizeType:Response_Data;
    }
}

action(f_sendInvToSharers, "f", desc="invalidate sharers for L2 replacement") {
    enqueue(L1RequestL2Network_out, RequestMsg, to_l1_latency) {
        DirEntry dir_entry := getDirEntry(address);
        assert(is_valid(dir_entry));
        assert(is_valid(tbe));
        DPRINTF(RubySlicc, "f_sendInvToSharers::addr=%x, dest=%s\n"
                         , address, dir_entry.Sharers);
        tbe.dirToEvict  := true;
        out_msg.addr := address;
        out_msg.Type := CoherenceRequestType:INV_EVC;
        out_msg.Requestor := machineID;
        out_msg.Destination := dir_entry.Sharers;
        out_msg.MessageSize := MessageSizeType:Request_Control;
    }
}

action(fw_sendFwdInvToSharers, "fw", desc="invalidate sharers for request") {
    peek(L1RequestL2Network_in, RequestMsg) {
        enqueue(L1RequestL2Network_out, RequestMsg, to_l1_latency) {
            DirEntry dir_entry := getDirEntry(address);
            assert(is_valid(cache_entry));
            assert(is_valid(dir_entry));
            out_msg.addr := address;
            out_msg.Type := CoherenceRequestType:INV;
            out_msg.Requestor := in_msg.Requestor;
            out_msg.Destination := dir_entry.Sharers;
            out_msg.MessageSize := MessageSizeType:Request_Control;
        }
    }
}

action(fwm_sendFwdInvToSharersMinusRequestor, "fwm", desc="invalidate sharers for request, requestor is sharer") {
    peek(L1RequestL2Network_in, RequestMsg) {
        enqueue(L1RequestL2Network_out, RequestMsg, to_l1_latency) {
            DirEntry dir_entry := getDirEntry(address);
            //assert(is_valid(cache_entry));
            assert(is_valid(dir_entry));
            out_msg.addr := address;
            out_msg.Type := CoherenceRequestType:INV;
            out_msg.Requestor := in_msg.Requestor;
            out_msg.Destination := dir_entry.Sharers;
            out_msg.Destination.remove(in_msg.Requestor);
            out_msg.MessageSize := MessageSizeType:Request_Control;
            DPRINTF(RubySlicc, "fwm_sendFwdInvToSharersMinusRequestor::addr=%x, sharers=%d\n"
                             , address, out_msg.Destination);
        }
    }
}

// OTHER ACTIONS
action(i_allocateTBE, "i", desc="Allocate TBE for request") {
    if (is_invalid(tbe)) {
        check_allocate(TBEs);
        DirEntry dir_entry := getDirEntry(address);
        State state := getState(tbe, cache_entry, address);
        //assert(is_valid(dir_entry));
        DPRINTF(RubySlicc, "i_allocateTBE::addr=%x, state=%d\n"
                        , address, state);

        TBEs.allocate(address);
        set_tbe(TBEs[address]);
        tbe.L1_GetS_IDs.clear();
        // must set state in advance
        tbe.TBEState := state;
        if (is_valid(cache_entry)) {
            tbe.DataBlk := cache_entry.DataBlk;
            tbe.Dirty := cache_entry.Dirty;
        }
        if (is_valid(dir_entry)) {
            tbe.pendingAcks := dir_entry.Sharers.count();
        }
    } else {
        DPRINTF(RubySlicc, "i_allocateTBE::has existed::addr=%x, state=%d\n"
                         , address, getState(tbe, cache_entry, address));
    }
}

action(s_deallocateTBE, "s", desc="Deallocate external TBE") {
    if (is_valid(tbe)) {
        TBEs.deallocate(address);
        unset_tbe();
    }
}

action(jj_popL1RequestQueue, "\j", desc="Pop incoming L1 request queue") {
    Tick delay := L1RequestL2Network_in.dequeue(clockEdge());
    profileMsgDelay(0, ticksToCycles(delay));
}

action(jjt_popTriggerPort, "\jjt", desc="Pop trigger queue") {
    Tick delay := triggerInPort.dequeue(clockEdge());
    profileMsgDelay(0, ticksToCycles(delay));
}

action(k_popUnblockQueue, "k", desc="Pop incoming unblock queue") {
    Tick delay := L1unblockNetwork_in.dequeue(clockEdge());
    profileMsgDelay(0, ticksToCycles(delay));
}

action(o_popIncomingResponseQueue, "o", desc="Pop Incoming Response queue") {
    Tick delay := responseL2Network_in.dequeue(clockEdge());
    profileMsgDelay(1, ticksToCycles(delay));
}

action(m_allocDataFromMem, "ad", desc="Allocate data from mem to tbe") {
    peek(responseL2Network_in, ResponseMsg) {
        DPRINTF(RubySlicc, "m_allocDataFromMem::addr=%x, datablk=%s\n"
                         , address, in_msg.DataBlk);
        assert(is_valid(tbe));
        tbe.DataBlk := in_msg.DataBlk;
    }
}

action(m_writeDataToCache, "m", desc="Write data from response queue to cache") {
    peek(responseL2Network_in, ResponseMsg) {
        assert(is_valid(cache_entry));
        cache_entry.DataBlk := in_msg.DataBlk;
        if (in_msg.Dirty) {
            cache_entry.Dirty := in_msg.Dirty;
        }
    }
}

action(mr_writeDataToCacheFromRequest, "mr", desc="Write data from request queue to cache") {
    peek(L1RequestL2Network_in, RequestMsg) {
        DPRINTF(RubySlicc, "mr_writeDataToCacheFromRequest::addr=%x, dirty=%d\n"
                         , in_msg.addr, in_msg.Dirty);
        if (in_msg.Dirty) {
            // need to allocate cache
            if (is_invalid(cache_entry)) {
                if (L2cache.cacheAvail(address)) {
                    set_cache_entry(L2cache.allocate(address, new CacheEntry));
                    cache_entry.DataBlk := in_msg.DataBlk;
                    cache_entry.Dirty := in_msg.Dirty;
                    L2cache.removePendingAddr(in_msg.addr);
                } else {
                    State orgState := getState(tbe, cache_entry, in_msg.addr);
                    // need replacement
                    Addr victim := L2cache.cacheProbe(in_msg.addr);
                    CacheEntry L2cache_entry := getCacheEntry(victim);
                    TBE tbe_victim := TBEs[victim];
                    DPRINTF(RubySlicc, "mr_writeDataToCacheFromRequest::Cache victim::addr=%x, state=%d, victim=%x, state=%d\n"
                                     , in_msg.addr, orgState, victim, getState(tbe_victim, L2cache_entry, victim));
                    // set dir_entry flag, to block other request
                    if (orgState == State:MT) {
                        DirEntry dir_entry := getDirEntry(address);
                        assert(is_valid(dir_entry));
                        dir_entry.waitEvict := true;
                    }
                    stall_and_wait(L1RequestL2Network_in, victim);
                    // trigger cache replacement by trigger port
                    enqueue(triggerOutPort, RequestMsg, 0) {
                        out_msg.addr := victim;
                        out_msg.Type := CoherenceRequestType:EVC;
                        out_msg.AccessMode := in_msg.AccessMode;
                        //out_msg.Requestor := in_msg.Requestor;
                        //out_msg.Destination := in_msg.Destination;
                        out_msg.MessageSize := in_msg.MessageSize;
                        out_msg.DataBlk := L2cache_entry.DataBlk;
                        //out_msg.Len := in_msg.Len;
                        out_msg.Dirty := L2cache_entry.Dirty;
                        //out_msg.Prefetch := in_msg.Prefetch;
                    }
                }
            } else {
                cache_entry.DataBlk := in_msg.DataBlk;
                cache_entry.Dirty := in_msg.Dirty;
                L2cache.removePendingAddr(in_msg.addr);
            }
        } else {
            // for clean write back, still need to remove
            // possible pending address
            L2cache.removePendingAddr(in_msg.addr);
        }
        if (is_valid(cache_entry)) {
            // trigger PUTX-end
            DPRINTF(RubySlicc, "mr_writeDataToCacheFromRequest::PUTX end::addr=%x\n"
                             , in_msg.addr);
            enqueue(triggerOutPort, RequestMsg, 0) {
                out_msg.addr := in_msg.addr;
                out_msg.Type := in_msg.Type;
                out_msg.AccessMode := in_msg.AccessMode;
                out_msg.Requestor := in_msg.Requestor;
                out_msg.Destination := in_msg.Destination;
                out_msg.MessageSize := in_msg.MessageSize;
                out_msg.DataBlk := in_msg.DataBlk;
                out_msg.Len := in_msg.Len;
                out_msg.Dirty := in_msg.Dirty;
                out_msg.Prefetch := in_msg.Prefetch;
            }
            // popL1RequestQueue 
            // Tick delay := L1RequestL2Network_in.dequeue(clockEdge());
            // profileMsgDelay(0, ticksToCycles(delay));
        }
    }
}

action(mr_writeDataToCacheFromRespond, "mrs", desc="Write data from response queue to cache") {
    peek(responseL2Network_in, ResponseMsg) {
        DPRINTF(RubySlicc, "mr_writeDataToCacheFromRespond::addr=%x, dirty=%d\n"
                         , in_msg.addr, in_msg.Dirty);
        // need to allocate cache
        if (is_invalid(cache_entry)) {
            // if (getState(tbe, cache_entry, address) == State:I_I && 
            //     isCacheToEvict(tbe) && !L2cache.cacheAvail(address)) {
            //     // here is the situation that directory eviction
            //     // encounter cache eviction, we deallocate cache
            //     // entry first and set state to I_I. However, the
            //     // directory EV creates back invalidation. Therefore,
            //     // we do not allocate this cache entry again when
            //     // cache is unavailable
            // } else {
            assert(L2cache.cacheAvail(address));
            set_cache_entry(L2cache.allocate(address, new CacheEntry));
            cache_entry.DataBlk := in_msg.DataBlk;
            cache_entry.Dirty := in_msg.Dirty;
            // }
            // } else {
            //     // need replacement
            //     assert(false);
            //     Addr victim := L2cache.cacheProbe(in_msg.addr);
            //     CacheEntry L2cache_entry := getCacheEntry(victim);
            //     TBE tbe_victim := TBEs[victim];
            //     DPRINTF(RubySlicc, "mr_writeDataToCacheFromRequest::Cache victim::addr=%x, state=%d, victim=%x, state=%d\n"
            //                     , in_msg.addr, getState(tbe, cache_entry, in_msg.addr), victim
            //                     , getState(tbe_victim, L2cache_entry, victim));
            //     stall_and_wait(responseL2Network_in, victim);
            //     enqueue(triggerOutPort, RequestMsg, 0) {
            //         out_msg.addr := victim;
            //         out_msg.Type := CoherenceRequestType:EVC;
            //         out_msg.AccessMode := in_msg.AccessMode;
            //         //out_msg.Requestor := in_msg.Requestor;
            //         //out_msg.Destination := in_msg.Destination;
            //         out_msg.MessageSize := in_msg.MessageSize;
            //         out_msg.DataBlk := L2cache_entry.DataBlk;
            //         //out_msg.Len := in_msg.Len;
            //         out_msg.Dirty := L2cache_entry.Dirty;
            //         //out_msg.Prefetch := in_msg.Prefetch;
            //     }
            // }
        } else {
            cache_entry.DataBlk := in_msg.DataBlk;
            cache_entry.Dirty := in_msg.Dirty;
        }
        // if (is_valid(tbe)) {
        //     State state := getState(tbe, cache_entry, address);
        //     DPRINTF(RubySlicc, "mr_writeDataToCacheFromRespond::remove waitDataNum::addr=%x, state=%d, waitDataNum=%d\n"
        //                      , address, state, tbe.waitDataNum);

        //     if ((state == State:MT_IIB || state == State:SS || state == State:MT_SB) && tbe.waitDataNum > 0) {
        //         tbe.waitDataNum := tbe.waitDataNum - 1;
        //     }
        // }
    }
}

action(c_checkCache, "ccc", desc="Check cache can be allocate before back invalidation") {
    //peek(L1RequestL2Network_in, RequestMsg) {
    //assert(is_valid(tbe));
    bool request_need_allocate_cache := needAllocateCache(getState(tbe, cache_entry, address), address, cache_entry);
    bool evict_need_allocate_cache := isCacheToEvict(tbe) || isDirToEvict(tbe);
    //assert(evict_need_allocate_cache || request_need_allocate_cache);
    DPRINTF(RubySlicc, "c_checkCache::addr=%x, evict_need_ac=%d, request_need_ac=%d\n"
                    , address, evict_need_allocate_cache, request_need_allocate_cache);

    if (!L2cache.cacheAvail(address) && (evict_need_allocate_cache || request_need_allocate_cache)) {
        Addr victim := L2cache.cacheProbe(address);
        CacheEntry L2cache_entry := getCacheEntry(victim);
        TBE tbe_victim := TBEs[victim];
        DPRINTF(RubySlicc, "c_checkCache::Cache victim::addr=%x, victim=%x, victim state=%d\n"
                        , address, victim, getState(tbe_victim, L2cache_entry, victim));
        //stall_and_wait(responseL2Network_in, victim);
        enqueue(triggerOutPort, RequestMsg, 0) {
            out_msg.addr := victim;
            out_msg.Type := CoherenceRequestType:EVC;
            //out_msg.AccessMode := in_msg.AccessMode;
            //out_msg.Requestor := in_msg.Requestor;
            //out_msg.Destination := in_msg.Destination;
            //out_msg.MessageSize := in_msg.MessageSize;
            out_msg.DataBlk := L2cache_entry.DataBlk;
            //out_msg.Len := in_msg.Len;
            out_msg.Dirty := L2cache_entry.Dirty;
            //out_msg.Prefetch := in_msg.Prefetch;
        }
        // directly allocate cache here
        // otherwise, the other request
        // will opccupy the cache line
        // set_cache_entry(L2cache.allocate(address, new CacheEntry));
        // save cache line for incoming request
        L2cache_entry.aheadAllocate := true;
        L2cache_entry.aheadAddr := address;

    } else if (evict_need_allocate_cache || request_need_allocate_cache) {
        // still need save pending address
        L2cache.savePendingAddr(address);
    }
    //}
}

// action(iw_insertAndWaitTriggerPort, "iw", desc="insert SF replacement into Trigger port and wait") {
//     enqueue(triggerOutPort, RequestMsg, 0) {
//         DPRINTF(RubySlicc, "iw_insertAndWaitTriggerPort::addr=%x\n", address);
//         out_msg.addr := address;
//         out_msg.Type := CoherenceRequestType:SF_EVC;
//     }
//     stall_and_wait(triggerInPort, address);
// }

action(q_updateAck, "q", desc="update pending ack count") {
    peek(responseL2Network_in, ResponseMsg) {
        assert(is_valid(tbe));
        assert(in_msg.AckCount < 2);
        tbe.pendingAcks := tbe.pendingAcks - 1;
        APPEND_TRANSITION_COMMENT(in_msg.AckCount);
        APPEND_TRANSITION_COMMENT(" p: ");
        APPEND_TRANSITION_COMMENT(tbe.pendingAcks);
    }
}

action(qq_writeDataToTBE, "\qq", desc="Write data from response queue to TBE") {
    peek(responseL2Network_in, ResponseMsg) {
        assert(is_valid(tbe));
        tbe.DataBlk := in_msg.DataBlk;
        tbe.Dirty := in_msg.Dirty;
    }
}

action(ss_recordGetSL1ID, "\s", desc="Record L1 GetS for load response") {
    peek(L1RequestL2Network_in, RequestMsg) {
        assert(is_valid(tbe));
        tbe.L1_GetS_IDs.add(in_msg.Requestor);
    }
}

action(xx_recordGetXL1ID, "\x", desc="Record L1 GetX for store response") {
    peek(L1RequestL2Network_in, RequestMsg) {
        assert(is_valid(tbe));
        tbe.L1_GetX_ID := in_msg.Requestor;
        DPRINTF(RubySlicc, "xx_recordGetXL1ID::addr=%x, GetX_ID=%d\n"
                         , address, in_msg.Requestor);
    }
}

action(set_setMRU, "\set", desc="set the MRU entry") {
    L2cache.setMRU(address);
    SF.setMRU(address);
}

action(qq_allocateL2CacheBlock, "\q", desc="Set L2 cache tag equal to tag of block B.") {
    DPRINTF(RubySlicc, "qq_allocateL2CacheBlock::addr=%x\n", address);
    assert(L2cache.cacheAvail(address));
    if (is_invalid(cache_entry)) {
        set_cache_entry(L2cache.allocate(address, new CacheEntry));
    }
}

action(qq_allocateDirBlock, "\qdir", desc="Set dir tag equal to tag of block B.") {
    DPRINTF(RubySlicc, "qq_allocateDirBlock::addr=%x\n", address);
    assert(SF.cacheAvail(address));
    DirEntry dir_entry := getDirEntry(address);
    if (is_invalid(dir_entry)) {
        assert(!SF.isTagPresent(address));
        SF.allocate(address, new DirEntry);
        SF.removePendingAddr(address);
    }
}

action(rr_deallocateL2CacheBlock, "\r", desc="Deallocate L2 cache block.  Sets the cache to not present, allowing a replacement in parallel with a fetch.") {
    bool isCacheEvict := false;
    if(triggerInPort.isReady(clockEdge())) {
        peek(triggerInPort, RequestMsg) {
            isCacheEvict := (in_msg.Type == CoherenceRequestType:EVC);
        }
    }
    assert(is_valid(tbe));
    assert(is_valid(cache_entry));
    // set tbe to be eivct
    // but may encounter BUG
    tbe.cacheToEvict := isCacheEvict;
    bool ahead_allocate := cache_entry.aheadAllocate;
    Addr ahead_addr := cache_entry.aheadAddr;
    DPRINTF(RubySlicc, "rr_deallocateL2CacheBlock::addr=%x, isCacheEvict=%d, allocaahead=%d, aheadAddr=%x\n"
                     , address, isCacheEvict, cache_entry.aheadAllocate, cache_entry.aheadAddr);
    L2cache.deallocate(address);
    unset_cache_entry();
    if (ahead_allocate) {
        //set_cache_entry(L2cache.allocate(ahead_addr, new CacheEntry));
        //L2cache.setMRU(ahead_addr);
        L2cache.savePendingAddr(ahead_addr);
    }
}

action(rr_deallocateDirBlock, "\rdir", desc="Deallocate dir.  Sets the dir to not present, allowing a replacement in parallel with a fetch.") {
    DPRINTF(RubySlicc, "rr_deallocateDirBlock::addr=%x\n"
                     , address);
    assert(SF.isTagPresent(address));
    SF.deallocate(address);
}

action(t_sendWBAck, "t", desc="Send writeback ACK") {
    peek(L1RequestL2Network_in, RequestMsg) {
        enqueue(responseL2Network_out, ResponseMsg, to_l1_latency) {
            out_msg.addr := address;
            out_msg.Type := CoherenceResponseType:WB_ACK;
            out_msg.Sender := machineID;
            out_msg.Destination.add(in_msg.Requestor);
            out_msg.MessageSize := MessageSizeType:Response_Control;
        }
    }
}

action(tt_sendSilentToMem, "ttm", desc="Send ACK_Silent to mem") {
    enqueue(responseL2Network_out, ResponseMsg, 1) {
        assert(is_valid(tbe));
        DPRINTF(RubySlicc, "tt_sendSilentToMem::addr=%d, hasSendDataToMem=%d\n"
                         , address, tbe.hasSendDataToMem);
        if(!tbe.hasSendDataToMem) {
            out_msg.addr := address;
            out_msg.Type := CoherenceResponseType:ACK_Silent;
            // ACK_Silent is only used here
            // to notice memory control that certain block is not maintained
            // by the upstream 
            out_msg.Sender := machineID;
            out_msg.Destination.add(mapAddressToMachine(address, MachineType:Directory));
            out_msg.MessageSize := MessageSizeType:Response_Control;
        }
    }
}

action(t_sendDataToMem, "tm", desc="Send data to mem when cache is unavail") {
    peek(responseL2Network_in, ResponseMsg) {
        enqueue(responseL2Network_out, ResponseMsg, l2_request_latency) {
            assert(in_msg.Dirty);
            assert(is_valid(tbe));
            if(!tbe.hasSendDataToMem) {
                tbe.hasSendDataToMem := true;
                out_msg.addr := address;
                out_msg.Type := CoherenceResponseType:MEMORY_DATA;
                out_msg.Sender := machineID;
                out_msg.Destination.add(mapAddressToMachine(address, MachineType:Directory));
                out_msg.DataBlk := in_msg.DataBlk;
                out_msg.Dirty := in_msg.Dirty;
                out_msg.MessageSize := MessageSizeType:Response_Data;
            }
        }
    }
}

action(ts_sendInvAckToUpgrader, "ts", desc="Send ACK to upgrader") {
    peek(L1RequestL2Network_in, RequestMsg) {
        enqueue(responseL2Network_out, ResponseMsg, to_l1_latency) {
            DirEntry dir_entry := getDirEntry(address);
            //assert(is_valid(cache_entry));
            assert(is_valid(dir_entry));
            DPRINTF(RubySlicc, "ts_sendInvAckToUpgrader::addr=%x, requestor=%d, dir_sharers=%d\n"
                             , address, in_msg.Requestor, dir_entry.Sharers);
            assert(dir_entry.Sharers.isElement(in_msg.Requestor));
            out_msg.addr := address;
            out_msg.Type := CoherenceResponseType:ACK;
            out_msg.Sender := machineID;
            out_msg.Destination.add(in_msg.Requestor);
            out_msg.MessageSize := MessageSizeType:Response_Control;
            // upgrader doesn't get ack from itself, hence the + 1
            out_msg.AckCount := 0 - dir_entry.Sharers.count() + 1;
        }
    }
}

action(uu_profileMiss, "\um", desc="Profile the demand miss") {
    L2cache.profileDemandMiss();
}

action(uu_profileHit, "\uh", desc="Profile the demand hit") {
    L2cache.profileDemandHit();
}

action(nn_addSharer, "\n", desc="Add L1 sharer to list") {
    peek(L1RequestL2Network_in, RequestMsg) {
        DirEntry dir_entry := getDirEntry(address);
        assert(is_valid(dir_entry));
        addSharer(address, in_msg.Requestor, dir_entry);
        APPEND_TRANSITION_COMMENT( dir_entry.Sharers );
    }
}

action(nnu_addSharerFromUnblock, "\nu", desc="Add L1 sharer to list") {
    peek(L1unblockNetwork_in, ResponseMsg) {
        DirEntry dir_entry := getDirEntry(address);
        DPRINTF(RubySlicc, "nnu_addSharerFromUnblock::addr=%x\n", address);
        //assert(is_valid(cache_entry));
        assert(is_valid(dir_entry));
        addSharer(address, in_msg.Sender, dir_entry);
    }
}

action(kk_removeRequestSharer, "\k", desc="Remove L1 Request sharer from list") {
    peek(L1RequestL2Network_in, RequestMsg) {
        DirEntry dir_entry := getDirEntry(address);
        bool is_sharer := isSharer(address, in_msg.Requestor, dir_entry);
        if (is_sharer) {
            dir_entry.Sharers.remove(in_msg.Requestor);
            // check whether need to remove dir_entry
            if (dir_entry.Sharers.count() == 0) {
                assert(SF.isTagPresent(address));
                // set state before remove
                if (in_msg.Dirty) {
                    dir_entry.DirState := State:M;
                } else {
                    dir_entry.DirState := State:NP;
                    //if (isDirty(cache_entry) && getState(tbe, cache_entry, address) == State:SS) {
                        // if state is SS and dirty, which means LLC do not maintain the
                        // RW permission, we need to silent update directory
                    enqueue(responseL2Network_out, ResponseMsg, 1) {
                        out_msg.addr := address;
                        out_msg.Type := CoherenceResponseType:ACK_Silent;
                        // ACK_Silent is only used here
                        // to notice memory control that certain block is not maintained
                        // by the upstream 
                        out_msg.Sender := machineID;
                        out_msg.Destination.add(mapAddressToMachine(address, MachineType:Directory));
                        out_msg.MessageSize := MessageSizeType:Response_Control;
                    }
                    //}
                }
                SF.deallocate(address);
            }
        }
        // remove wait victim eviction flag
        if (is_valid(dir_entry)) {
            dir_entry.waitEvict := false;
        }
        DPRINTF(RubySlicc, "kk_removeRequestSharer::addr=%x, requestor=%d, is_sharer=%d\n"
                         , address, in_msg.Requestor, is_sharer);
    }
}

// action(ke_resetExclusive, "ke", desc="reset Exclusive") {
//     DirEntry dir_entry := getDirEntry(address);
//     assert(is_valid(dir_entry));
//     assert(dir_entry.Sharers.count() == 1);
//     DPRINTF(RubySlicc, "ke_resetExclusive::addr=%x, pre_excl=%d, new_excl=%d\n"
//                      , address, dir_entry.Exclusive
//                      , dir_entry.Sharers.smallestElement());
//     if (!dir_entry.Sharers.isElement(dir_entry.Exclusive)) {
//         dir_entry.Exclusive := dir_entry.Sharers.smallestElement();
//     }
// }

action(ll_clearSharers, "\l", desc="Remove all L1 sharers from list") {
    peek(L1RequestL2Network_in, RequestMsg) {
        DirEntry dir_entry := getDirEntry(address);
        assert(is_valid(dir_entry));
        if (dir_entry.DirState == State:MT) {
            assert(dir_entry.Sharers.count() == 1);
        }
        dir_entry.Sharers.clear();
    }
}

action(mm_markExclusive, "\m", desc="set the exclusive owner") {
    peek(L1RequestL2Network_in, RequestMsg) {
        DirEntry dir_entry := getDirEntry(address);
        assert(is_valid(cache_entry));
        assert(is_valid(dir_entry));
        dir_entry.Sharers.clear();
        dir_entry.Exclusive := in_msg.Requestor;
        addSharer(address, in_msg.Requestor, dir_entry);
    }
}

action(mmd_markExclusiveFromDir, "\md", desc="set the exclusive owner") {
    DirEntry dir_entry := getDirEntry(address);
    assert(is_valid(dir_entry));
    assert(dir_entry.Sharers.count() == 1);
    //dir_entry.Sharers.clear();
    dir_entry.Exclusive := dir_entry.Sharers.smallestElement();
    //addSharer(address, in_msg.Sender, dir_entry);
    DPRINTF(RubySlicc, "mmd_markExclusiveFromDir::addr=%x, exclusive=%d\n"
                        , address, dir_entry.Exclusive);
}

action(mmu_markExclusiveFromUnblock, "\mu", desc="set the exclusive owner") {
    peek(L1unblockNetwork_in, ResponseMsg) {
        DPRINTF(RubySlicc, "mmu_markExclusiveFromUnblock::addr=%x, exclusive=%d\n"
                         , address, in_msg.Sender);
        DirEntry dir_entry := getDirEntry(address);
        //assert(is_valid(cache_entry));
        assert(is_valid(dir_entry));
        dir_entry.Sharers.clear();
        dir_entry.Exclusive := in_msg.Sender;
        addSharer(address, in_msg.Sender, dir_entry);
    }
}

action(mcd_markCacheDirty, "\mcd", desc="mark cache as dirty") {
    assert(is_valid(cache_entry));
    cache_entry.Dirty := true;
    DPRINTF(RubySlicc, "mcd_markCacheDirty::addr=%x, data=%s\n"
                     , address, cache_entry.DataBlk);
}

action(rp_removeCachePendingAddr, "rp", desc="remove pending address") {
    L2cache.removePendingAddr(address);
}

action(zz_stallAndWaitL1RequestQueue, "zz", desc="recycle L1 request queue") {
    stall_and_wait(L1RequestL2Network_in, address);
}

action(zzt_stallAndWaitTriggerQueue, "zzt", desc="recycle L1 request queue") {
    stall_and_wait(triggerInPort, address);
}

action(zn_recycleResponseNetwork, "zn", desc="recycle memory request") {
    responseL2Network_in.recycle(clockEdge(), cyclesToTicks(recycle_latency));
}

action(kd_wakeUpDependents, "kd", desc="wake-up dependents") {
    wakeUpBuffers(address);
}

//*****************************************************
// TRANSITIONS
//*****************************************************


//===============================================
// BASE STATE - I

// Transitions from I (Idle)
// transition({NP, IS, ISS, IM, SS, M, M_I, I_I, S_I, MT_IB, MT_SB}, L1_PUTX) {
//     t_sendWBAck;
//     jj_popL1RequestQueue;
// }

// transition({NP, SS, M, MT, M_I, I_I, S_I, IS, ISS, IM, MT_IB, MT_SB}, L1_PUTX_old) {
//     t_sendWBAck;
//     jj_popL1RequestQueue;
// }
transition({M_I, MT_I, MCT_I, I_I, S_I, ISS, IS, IM, SS_MB, MT_MB, MT_IIB, MT_IB, MT_SB},  
           {L1_PUTX_front, L1_PUTX_end, L1_PUTX_clean}){
    zz_stallAndWaitL1RequestQueue;
}

transition(MT, Pend){
    zz_stallAndWaitL1RequestQueue;
}

transition({NP,M,S_I,I_I}, Pop_ReqIn){
}

transition({MT_I, MCT_I, I_I, S_I, ISS, IS, IM, SS_MB, MT_MB, MT_IIB, MT_IB, MT_SB},  
           {SF_Replacement_withC, SF_Replacement}){
    zzt_stallAndWaitTriggerQueue;
}

transition({NP, SS, M, MT}, L1_PUTX_front) {
    mr_writeDataToCacheFromRequest;
}

transition({NP, SS, M, MT}, L1_PUTX_end, M) {
    // ll_clearSharers;
    kk_removeRequestSharer;
    // for directory sharers has been clean,
    // we drop directory entry here.
    t_sendWBAck;
    // mark dirty
    // mcd_markCacheDirty;
    jj_popL1RequestQueue;
    jjt_popTriggerPort;
    kd_wakeUpDependents;
}

transition(SS, L1_PUTX_end_toSS) {
    kk_removeRequestSharer;
    t_sendWBAck;
    jj_popL1RequestQueue;
    jjt_popTriggerPort;
    kd_wakeUpDependents;
}

transition({SS,MT}, L1_PUTX_end_toMT, MT) {
    // add MT, this is the situation that
    // GETX and PUTX happen at the same time
    // GETX will make state to MT, but PUTX
    // also need to switch to MT after remove
    // sharers, so that add MT
    kk_removeRequestSharer;
    mmd_markExclusiveFromDir;
    t_sendWBAck;
    jj_popL1RequestQueue;
    jjt_popTriggerPort;
    kd_wakeUpDependents;
}

transition({NP, SS, M}, L1_PUTX_clean) {
    // ll_clearSharers;
    kk_removeRequestSharer;
    t_sendWBAck;
    jj_popL1RequestQueue;
    kd_wakeUpDependents;
}

transition({SS,MT}, L1_PUTX_clean_toM, M) {
    kk_removeRequestSharer;
    t_sendWBAck;
    jj_popL1RequestQueue;
    kd_wakeUpDependents;
}

transition({SS,MT}, L1_PUTX_clean_pop) {
    kk_removeRequestSharer;
    t_sendWBAck;
    jj_popL1RequestQueue;
    kd_wakeUpDependents;
}

transition({SS,MT}, L1_PUTX_clean_toNP, NP) {
    kk_removeRequestSharer;
    t_sendWBAck;
    jj_popL1RequestQueue;
    kd_wakeUpDependents;
}

transition({SS,M,MT}, L2_Replacement, M_I) {
    i_allocateTBE;
    c_exclusiveReplacement;
    rr_deallocateL2CacheBlock;
    jjt_popTriggerPort;
    //rr_deallocateDirBlock;
}

transition(S_I, L2_Replacement, I_I) {
    // dir eviction encounters cache eviction
    c_exclusiveReplacement;
    rr_deallocateL2CacheBlock;
    jjt_popTriggerPort;
    //rr_deallocateDirBlock;
}

transition({M,MT,SS}, SF_Replacement, I_I) {
    // directory evicts without cache
    i_allocateTBE;
    f_sendInvToSharers;
    rr_deallocateDirBlock;
    c_checkCache;
    // Once cache is not avail
    // we do not pend current
    // directory eviction
    jjt_popTriggerPort;
}

transition({S_I,I_I}, SF_FakeReplacement) {
    // directory has been evicted once
    jjt_popTriggerPort;
}

transition(M_I, SF_Replacement, I_I) {
    // cache eviction but enconter SF
    // evicts the same address
    f_sendInvToSharers;
    rr_deallocateDirBlock;
    c_checkCache;
    jjt_popTriggerPort;
}

transition({M,MT,SS}, SF_Replacement_withC, S_I) {
    // directory evicts with cache
    i_allocateTBE;
    set_setMRU; 
    // this is to enhance the priority of cache line
    f_sendInvToSharers;
    rr_deallocateDirBlock;
    c_checkCache;
    jjt_popTriggerPort;
}

// transition(SS, SF_Replacement, I_I) {
//     i_allocateTBE;
//     f_sendInvToSharers;
//     rr_deallocateDirBlock;
// }

transition({I_I, S_I}, Ack) {
    q_updateAck;
    o_popIncomingResponseQueue;
}

transition({I_I,S_I}, WB_Data, S_I) {
    q_updateAck;
    mr_writeDataToCacheFromRespond;
    rp_removeCachePendingAddr;
    o_popIncomingResponseQueue;
}

transition(I_I, WB_Data_Keep) {
    q_updateAck;
    // may need to send data to memory
    t_sendDataToMem;
    rp_removeCachePendingAddr;
    o_popIncomingResponseQueue;
}

transition(I_I, Ack_all, NP) {
    tt_sendSilentToMem;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    rp_removeCachePendingAddr;
    kd_wakeUpDependents;
}

transition(I_I, Ack_all_DtoMem, NP) {
    t_sendDataToMem;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    rp_removeCachePendingAddr;
    kd_wakeUpDependents;
}

transition(S_I, Ack_all, M) {
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    rp_removeCachePendingAddr;
    kd_wakeUpDependents;
}

transition({I_I,S_I}, WB_Data_all, M) {
    mr_writeDataToCacheFromRespond;
    rp_removeCachePendingAddr;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
}

transition({IM, IS, ISS, SS_MB, MT_MB, MT_IIB, MT_IB, MT_SB}, {L2_Replacement}) {
    //zz_stallAndWaitL1RequestQueue;
    zzt_stallAndWaitTriggerQueue;
}

transition({IM, IS, ISS, SS_MB, MT_MB, MT_IIB, MT_IB, MT_SB}, MEM_Inv) {
    zn_recycleResponseNetwork;
}

transition({I_I, S_I, M_I, MT_I, MCT_I, NP}, MEM_Inv) {
    o_popIncomingResponseQueue;
}

transition({SS_MB, MT_MB, MT_IIB, MT_IB, MT_SB}, {L1_GETS, L1_GET_INSTR, L1_GETX, L1_UPGRADE}) {
    zz_stallAndWaitL1RequestQueue;
}

transition(NP, {L1_GETS, L1_GET_INSTR},  ISS) {
    //qq_allocateL2CacheBlock;
    qq_allocateDirBlock;
    ll_clearSharers;
    nn_addSharer;
    i_allocateTBE;
    ss_recordGetSL1ID;
    a_issueFetchToMemory;
    uu_profileMiss;
    jj_popL1RequestQueue;
}

// transition(NP, L1_GET_INSTR, IS) {
//     //qq_allocateL2CacheBlock;
//     qq_allocateDirBlock;
//     ll_clearSharers;
//     nn_addSharer;
//     i_allocateTBE;
//     ss_recordGetSL1ID;
//     a_issueFetchToMemory;
//     uu_profileMiss;
//     jj_popL1RequestQueue;
// }

transition(NP, L1_GETX, IM) {
    //qq_allocateL2CacheBlock;
    qq_allocateDirBlock;
    ll_clearSharers;
    // nn_addSharer;
    i_allocateTBE;
    xx_recordGetXL1ID;
    a_issueFetchToMemory;
    uu_profileMiss;
    jj_popL1RequestQueue;
}


// transitions from IS/IM
transition(ISS, Mem_Data, MT_MB) {
    m_allocDataFromMem;
    ex_sendExclusiveDataToGetSRequestors;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
}

transition(IS, Mem_Data, SS) {
    m_allocDataFromMem;
    // allocate data into cache before
    // enter SS state
    mr_writeDataToCacheFromRespond;
    rp_removeCachePendingAddr;
    e_sendDataToGetSRequestors;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
}

transition(IM, Mem_Data, MT_MB) {
    m_allocDataFromMem;
    ee_sendDataToGetXRequestor;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
}

transition({IS, ISS}, {L1_GETS, L1_GET_INSTR}, IS) {
    // TODO: need to make sure
    // cache is avail
    c_checkCache;
    nn_addSharer;
    ss_recordGetSL1ID;
    uu_profileMiss;
    jj_popL1RequestQueue;
}

transition({IS, ISS}, L1_GETX) {
    zz_stallAndWaitL1RequestQueue;
}

transition(IM, {L1_GETX, L1_GETS, L1_GET_INSTR}) {
    zz_stallAndWaitL1RequestQueue;
}

// transitions from SS
transition(SS, {L1_GETS, L1_GET_INSTR}) {
    i_allocateTBE;
    ds_sendSharedDataToRequestor;
    nn_addSharer;
    c_checkCache;
    set_setMRU;
    uu_profileHit;
    jj_popL1RequestQueue;
}


transition(SS, L1_GETX, SS_MB) {
    d_sendDataToRequestor;
    fwm_sendFwdInvToSharersMinusRequestor;
    set_setMRU;
    uu_profileHit;
    jj_popL1RequestQueue;
}

transition({SS,M,MT}, L1_UPGRADE, SS_MB) {
    fwm_sendFwdInvToSharersMinusRequestor;
    ts_sendInvAckToUpgrader;
    set_setMRU;
    uu_profileHit;
    jj_popL1RequestQueue;
}

// transition(SS, L2_Replacement_clean, I_I) {
//     i_allocateTBE;
//     f_sendInvToSharers;
//     rr_deallocateL2CacheBlock;
//     rr_deallocateDirBlock;
// }

// transition(SS, {L2_Replacement, MEM_Inv}, S_I) {
//     i_allocateTBE;
//     f_sendInvToSharers;
//     rr_deallocateL2CacheBlock;
//     rr_deallocateDirBlock;
// }

transition(M, L1_GETX, MT_MB) {
    d_sendDataToRequestor;
    set_setMRU;
    uu_profileHit;
    jj_popL1RequestQueue;
}

transition(M, L1_GET_INSTR, SS) {
    d_sendDataToRequestor;
    nn_addSharer;
    set_setMRU;
    uu_profileHit;
    jj_popL1RequestQueue;
}

transition(M, L1_GETS, MT_MB) {
    dd_sendExclusiveDataToRequestor;
    set_setMRU;
    uu_profileHit;
    jj_popL1RequestQueue;
}

transition({SS,M,MT},  MEM_Inv, M_I) {
    i_allocateTBE;
    c_exclusiveReplacement;
    rr_deallocateL2CacheBlock;
    //rr_deallocateDirBlock;
}

// transition({SS,M,MT}, L2_Replacement_clean, M_I) {
//     i_allocateTBE;
//     c_exclusiveCleanReplacement;
//     rr_deallocateL2CacheBlock;
//     jjt_popTriggerPort;
//     //rr_deallocateDirBlock;
// }


// transitions from MT
transition(MT, L1_GETX, MT_MB) {
    b_forwardRequestToExclusive;
    uu_profileMiss;
    set_setMRU;
    jj_popL1RequestQueue;
}

transition(MT, {L1_GETS, L1_GET_INSTR}, MT_IIB) {
    b_forwardRequestToExclusive;
    c_checkCache; // need to check cache allovcation
    uu_profileMiss;
    set_setMRU;
    jj_popL1RequestQueue;
}

// transition(MT, {L2_Replacement, MEM_Inv}, MT_I) {
//     i_allocateTBE;
//     f_sendInvToSharers;
//     rr_deallocateL2CacheBlock;
//     rr_deallocateDirBlock;
// }

// transition(MT, L2_Replacement_clean, MCT_I) {
//     i_allocateTBE;
//     f_sendInvToSharers;
//     rr_deallocateL2CacheBlock;
//     rr_deallocateDirBlock;
// }

transition({SS_MB,MT_MB,MT_IIB}, Exclusive_Unblock, MT) {
    // update actual directory
    mmu_markExclusiveFromUnblock;
    k_popUnblockQueue;
    kd_wakeUpDependents;
}

// transition(SS_MB, {WB_Data, WB_Data_clean}) {
//     // this is the upgrade encounter L1 eviction
//     // we allocate cache but do not change state
//     // which MAYBE FAULT!!
//     mr_writeDataToCacheFromRespond;
//     rp_removeCachePendingAddr;
//     o_popIncomingResponseQueue;
// }

// transition(MT_IIB, {L1_PUTX, L1_PUTX_old}){
//     zz_stallAndWaitL1RequestQueue;
// }

transition({MT_IIB,SS}, Unblock, MT_IB) {
    nnu_addSharerFromUnblock;
    k_popUnblockQueue;
}

transition({MT_IIB,SS,MT_SB}, {WB_Data, WB_Data_clean}, MT_SB) {
    // add MT_SB in the state is for the
    // situation that two requestor acccess
    // at the same time, so there are two
    // unblock can be received
    mr_writeDataToCacheFromRespond;
    rp_removeCachePendingAddr;
    o_popIncomingResponseQueue;
}

transition(MT_IB, {WB_Data, WB_Data_clean}, SS) {
    //m_writeDataToCache;
    mr_writeDataToCacheFromRespond;
    rp_removeCachePendingAddr;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
}

transition(MT_SB, Unblock, SS) {
    nnu_addSharerFromUnblock;
    s_deallocateTBE;
    k_popUnblockQueue;
    kd_wakeUpDependents;
}

transition(MT_SB, Unblock_Keep) {
    nnu_addSharerFromUnblock;
    k_popUnblockQueue;
}

// writeback states
transition({I_I, S_I, MT_I, MCT_I, M_I}, {L1_GETX, L1_UPGRADE, L1_GETS, L1_GET_INSTR}) {
    zz_stallAndWaitL1RequestQueue;
}

// transition(I_I, Ack) {
//     q_updateAck;
//     o_popIncomingResponseQueue;
// }

// transition(I_I, Ack_all, M_I) {
//     c_exclusiveCleanReplacement;
//     o_popIncomingResponseQueue;
// }

transition({MT_I, MCT_I}, WB_Data, M_I) {
    qq_writeDataToTBE;
    ct_exclusiveReplacementFromTBE;
    o_popIncomingResponseQueue;
}

transition(MCT_I, {WB_Data_clean, Ack_all}, M_I) {
    c_exclusiveCleanReplacement;
    o_popIncomingResponseQueue;
}

// transition(MCT_I,  {L1_PUTX, L1_PUTX_old}){
//     zz_stallAndWaitL1RequestQueue;
// }

// L1 never changed Dirty data
transition(MT_I, {WB_Data_clean, Ack_all}, M_I) {
    ct_exclusiveReplacementFromTBE;
    o_popIncomingResponseQueue;
}

// transition(MT_I, {L1_PUTX, L1_PUTX_old}){
//     zz_stallAndWaitL1RequestQueue;
// }

// // possible race between unblock and immediate replacement
// transition({MT_MB,SS_MB}, {L1_PUTX, L1_PUTX_old}) {
//     zz_stallAndWaitL1RequestQueue;
// }

// transition(S_I, Ack) {
//     q_updateAck;
//     o_popIncomingResponseQueue;
// }

// transition(S_I, Ack_all, M_I) {
//     ct_exclusiveReplacementFromTBE;
//     o_popIncomingResponseQueue;
// }

transition(M_I, Mem_Ack, NP) {
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
}

transition({S_I,I_I,NP,M}, Mem_Ack) {
    // this is for cache eviction but
    // encouters SF eviction
    o_popIncomingResponseQueue;
}

transition(M_I, Mem_Ack_MT, MT) {
    mmd_markExclusiveFromDir;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
}

transition(M_I, Mem_Ack_SS, SS) {
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
}