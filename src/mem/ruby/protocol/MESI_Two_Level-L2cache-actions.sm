////////////////////////////////////////////////////////////////////////////
// MESI-cache actions definitions
////////////////////////////////////////////////////////////////////////////
// ACTIONS

action(a_issueFetchToMemory, "a", desc="fetch data from memory") {
    peek(L1RequestL2Network_in, RequestMsg) {
        enqueue(DirRequestL2Network_out, RequestMsg, l2_request_latency) {
            DPRINTF(RubySlicc, "a_issueFetchToMemory::addr=%x, Requestor=%d\n"
                             , address, machineID);
            out_msg.addr := address;
            out_msg.Type := CoherenceRequestType:GETS;
            out_msg.Requestor := machineID;
            out_msg.Destination.add(mapAddressToMachine(address, MachineType:Directory));
            out_msg.MessageSize := MessageSizeType:Control;
            out_msg.AccessMode := in_msg.AccessMode;
            out_msg.Prefetch := in_msg.Prefetch;
        }
    }
}

action(b_forwardRequestToExclusive, "b", desc="Forward request to the exclusive L1") {
    peek(L1RequestL2Network_in, RequestMsg) {
        if(is_invalid(getDirEntry(address))) {
            // allocate directory entry
            assert(!SF.isTagPresent(address));
            // SF must be available
            assert(SF.cacheAvail(address));
            SF.allocate(address, new DirEntry);
            addSharer(address, in_msg.Requestor, getDirEntry(address));
        }
        DirEntry dir_entry := getDirEntry(address);
        DPRINTF(RubySlicc, "b_forwardRequestToExclusive::addr=%x, requestor=%d, dir count=%d, cache valid=%d\n"
                         , address, in_msg.Requestor, dir_entry.Sharers.count(), is_valid(cache_entry));

        assert(dir_entry.Sharers.count() == 1);
        if (dir_entry.Sharers.isElement(in_msg.Requestor)) {
            // this is the situation that cache is avail
            assert(is_valid(cache_entry));
            // but directory is invalid
            enqueue(responseL2Network_out, ResponseMsg, l2_response_latency) {
                out_msg.addr := address;
                if (in_msg.Type == CoherenceRequestType:GETS ||
                    in_msg.Type == CoherenceRequestType:GET_INSTR) {
                    out_msg.Type := CoherenceResponseType:DATA_EXCLUSIVE;
                } else {
                    assert(in_msg.Type == CoherenceRequestType:GETX);
                    out_msg.Type := CoherenceResponseType:DATA;
                }
                out_msg.Sender := machineID;
                out_msg.Destination.add(in_msg.Requestor);
                out_msg.DataBlk := cache_entry.DataBlk;
                out_msg.MessageSize := MessageSizeType:Response_Data;
                out_msg.AckCount := 0 - dir_entry.Sharers.count();
                out_msg.AckCount := out_msg.AckCount + 1;
            }
        } else {
            enqueue(L1RequestL2Network_out, RequestMsg, to_l1_latency) {
                // formal
                out_msg.addr := address;
                out_msg.Type := in_msg.Type;
                out_msg.Requestor := in_msg.Requestor;
                out_msg.Destination.add(dir_entry.Exclusive);
                out_msg.MessageSize := MessageSizeType:Request_Control;
            }
        }
    }
}

action(c_exclusiveReplacement, "c", desc="Send data to memory") {
    assert(is_valid(cache_entry));
    DirEntry dir_entry := getDirEntry(address);
    bool isCacheEvict := false;
    int CacheEvictType := 0;
    if(triggerInPort.isReady(clockEdge())) {
        peek(triggerInPort, RequestMsg) {
            isCacheEvict := (in_msg.Type == CoherenceRequestType:EVC);
            CacheEvictType := in_msg.Len;
        }
    }
    //counter
    if (isCacheEvict) {
        L2cache.profileEvictions();
        L2cache.profileEvictionsType(CacheEvictType);
    }
    DPRINTF(RubySlicc, "c_exclusiveReplacement::addr=%x, dir valid=%d, dirty=%d, isCacheEvict=%d, data=%s\n"
                     , address, is_valid(dir_entry), cache_entry.Dirty, isCacheEvict, cache_entry.DataBlk);
    enqueue(responseL2Network_out, ResponseMsg, l2_response_latency) {
        if (is_invalid(dir_entry)) {
            // the upstream does not have the block, so the 
            // eviction should write back to mem
            if (!isDirty(cache_entry) && isCacheEvict) {
                out_msg.addr := address;
                out_msg.Type := CoherenceResponseType:ACK;
                out_msg.Sender := machineID;
                out_msg.Destination.add(mapAddressToMachine(address, MachineType:Directory));
                out_msg.MessageSize := MessageSizeType:Response_Control;
            } else {
                out_msg.addr := address;
                out_msg.Type := CoherenceResponseType:MEMORY_DATA;
                out_msg.Sender := machineID;
                out_msg.Destination.add(mapAddressToMachine(address, MachineType:Directory));
                out_msg.DataBlk := cache_entry.DataBlk;
                out_msg.Dirty := cache_entry.Dirty;
                out_msg.MessageSize := MessageSizeType:Response_Data;
            }
        } else {
            out_msg.addr := address;
            out_msg.Type := CoherenceResponseType:ACK_No_to_Mem;
            out_msg.Sender := machineID;
            out_msg.Destination.add(mapAddressToMachine(address, MachineType:Directory));
            out_msg.MessageSize := MessageSizeType:Response_Control;
        }
    }
}

// action(c_exclusiveCleanReplacement, "cc", desc="Send ack to memory for clean replacement") {
//     enqueue(responseL2Network_out, ResponseMsg, l2_response_latency) {
//         DPRINTF(RubySlicc, "c_exclusiveCleanReplacement::addr=%x\n", address);
//         out_msg.addr := address;
//         out_msg.Type := CoherenceResponseType:ACK;
//         out_msg.Sender := machineID;
//         out_msg.Destination.add(mapAddressToMachine(address, MachineType:Directory));
//         out_msg.MessageSize := MessageSizeType:Response_Control;
//     }
// }

// action(ct_exclusiveReplacementFromTBE, "ct", desc="Send data to memory") {
//     enqueue(responseL2Network_out, ResponseMsg, l2_response_latency) {
//         assert(is_valid(tbe));
//         out_msg.addr := address;
//         out_msg.Type := CoherenceResponseType:MEMORY_DATA;
//         out_msg.Sender := machineID;
//         out_msg.Destination.add(mapAddressToMachine(address, MachineType:Directory));
//         out_msg.DataBlk := tbe.DataBlk;
//         out_msg.Dirty := tbe.Dirty;
//         out_msg.MessageSize := MessageSizeType:Response_Data;
//     }
// }

action(d_sendDataToRequestor, "d", desc="Send data from cache to reqeustor") {
    peek(L1RequestL2Network_in, RequestMsg) {
        DPRINTF(RubySlicc, "d_sendDataToRequestor::addr=%x, Type=%d\n"
                         , address, in_msg.Type);
        if (is_valid(cache_entry)) {
            enqueue(responseL2Network_out, ResponseMsg, l2_response_latency) {
                out_msg.addr := address;
                out_msg.Type := CoherenceResponseType:DATA;
                out_msg.Sender := machineID;
                out_msg.Destination.add(in_msg.Requestor);
                out_msg.DataBlk := cache_entry.DataBlk;
                out_msg.MessageSize := MessageSizeType:Response_Data;
                if(is_invalid(getDirEntry(address))) {
                    // allocate directory entry
                    assert(!SF.isTagPresent(address));
                    // SF must be available
                    assert(SF.cacheAvail(address));
                    SF.allocate(address, new DirEntry);
                    addSharer(address, in_msg.Requestor, getDirEntry(address));
                }

                DirEntry dir_entry := getDirEntry(address);
                DPRINTF(RubySlicc, "d_sendDataToRequestor::addr=%x, Sharers=%s\n"
                                 , address, dir_entry.Sharers);
                out_msg.AckCount := 0 - dir_entry.Sharers.count();
                if (dir_entry.Sharers.isElement(in_msg.Requestor)) {
                    out_msg.AckCount := out_msg.AckCount + 1;
                }
            }
        } else {
            assert(getState(tbe, cache_entry, address) == State:SS);
            assert(is_valid((getDirEntry(address))));
            assert(in_msg.Type == CoherenceRequestType:GETX);
            DirEntry dir_entry := getDirEntry(address);
            enqueue(L1RequestL2Network_out, RequestMsg, 1) {
                // if cache is invalid, so that we need to send
                // request to upsteam
                out_msg.addr := address;
                out_msg.Type := CoherenceRequestType:GETX;
                out_msg.Requestor := in_msg.Requestor;
                out_msg.Destination.add(dir_entry.Sharers.smallestElement());
                out_msg.MessageSize := MessageSizeType:Request_Control;

            }
            enqueue(responseL2Network_out, ResponseMsg, 1) {
                DPRINTF(RubySlicc, "d_sendDataToRequestor::send wait_ack to requestor::addr=%x, requestor=%d, dir_sharers=%d\n"
                                    , address, in_msg.Requestor, dir_entry.Sharers);
                assert(!dir_entry.Sharers.isElement(in_msg.Requestor));
                out_msg.addr := address;
                out_msg.Type := CoherenceResponseType:ACK;
                out_msg.Sender := machineID;
                out_msg.Destination.add(in_msg.Requestor);
                out_msg.MessageSize := MessageSizeType:Response_Control;
                // upgrader doesn't get ack from itself, hence the + 1
                out_msg.AckCount := 0 - dir_entry.Sharers.count();
            }
        }
    }
}

action(dbk_sendDataToRequestorforBk, "dbk", desc="Send data from cache to reqeustor") {
    peek(L1RequestL2Network_in, RequestMsg) {
        DPRINTF(RubySlicc, "dbk_sendDataToRequestorforBk::addr=%x, Type=%d\n"
                         , address, in_msg.Type);
        assert(is_valid(cache_entry));
        assert(is_invalid(getDirEntry(address)));
        enqueue(responseL2Network_out, ResponseMsg, l2_response_latency) {
            out_msg.addr := address;
            if (in_msg.Type == CoherenceRequestType:GETX || 
                in_msg.Type == CoherenceRequestType:UPGRADE) {
                out_msg.Type := CoherenceResponseType:DATA;
            } else {
                out_msg.Type := CoherenceResponseType:DATA_EXCLUSIVE;
            }
            out_msg.Sender := machineID;
            out_msg.Destination.add(in_msg.Requestor);
            out_msg.DataBlk := cache_entry.DataBlk;
            out_msg.Dirty := cache_entry.Dirty;
            out_msg.MessageSize := MessageSizeType:Response_Data;
        }
        // remove cache entry for space to back up
        L2cache.deallocate(address);
        unset_cache_entry();
    }
}

action(dd_sendExclusiveDataToRequestor, "dd", desc="Send data from cache to reqeustor") {
    peek(L1RequestL2Network_in, RequestMsg) {
        enqueue(responseL2Network_out, ResponseMsg, l2_response_latency) {
            assert(is_valid(cache_entry));
            DPRINTF(RubySlicc, "dd_sendExclusiveDataToRequestor::addr=%x, dirty=%d, data=%s\n"
                             , address, cache_entry.Dirty, cache_entry.DataBlk);
            out_msg.addr := address;
            out_msg.Type := CoherenceResponseType:DATA_EXCLUSIVE;
            out_msg.Sender := machineID;
            out_msg.Destination.add(in_msg.Requestor);
            out_msg.DataBlk := cache_entry.DataBlk;
            out_msg.Dirty := cache_entry.Dirty;
            out_msg.MessageSize := MessageSizeType:Response_Data;
            if(is_invalid(getDirEntry(address))) {
                // allocate directory entry
                assert(!SF.isTagPresent(address));
                SF.allocate(address, new DirEntry);
                addSharer(address, in_msg.Requestor, getDirEntry(address));
            }

            DirEntry dir_entry := getDirEntry(address);
            out_msg.AckCount := 0 - dir_entry.Sharers.count();
            if (dir_entry.Sharers.isElement(in_msg.Requestor)) {
                out_msg.AckCount := out_msg.AckCount + 1;
            }
        }
    }
}

action(ds_sendSharedDataToRequestor, "ds", desc="Send data from cache to reqeustor") {
    peek(L1RequestL2Network_in, RequestMsg) {
        DirEntry dir_entry := getDirEntry(address);
        assert(is_valid(tbe));
        DPRINTF(RubySlicc, "ds_sendSharedDataToRequestor::addr=%x, cache valid=%d, dir valid=%d, waitDataNum=%d\n"
                         , address, is_valid(cache_entry), is_valid(dir_entry), tbe.waitDataNum);
        if (is_valid(cache_entry)) {
            enqueue(responseL2Network_out, ResponseMsg, l2_response_latency) {
                out_msg.addr := address;
                out_msg.Type := CoherenceResponseType:DATA;
                out_msg.Sender := machineID;
                out_msg.Destination.add(in_msg.Requestor);
                out_msg.DataBlk := cache_entry.DataBlk;
                out_msg.Dirty := cache_entry.Dirty;
                out_msg.MessageSize := MessageSizeType:Response_Data;
                out_msg.AckCount := 0;
            }
            DPRINTF(RubySlicc, "ds_sendSharedDataToRequestor::has cache line, so deallocate tbe\n");
            TBEs.deallocate(address);
            unset_tbe();
        } else {
            tbe.waitDataNum := tbe.waitDataNum + 1;
            enqueue(L1RequestL2Network_out, RequestMsg, to_l1_latency) {
                assert(is_valid(dir_entry));
                //assert(dir_entry.Sharers.count() > 1);
                out_msg.addr := address;
                out_msg.Type := in_msg.Type;
                out_msg.Requestor := in_msg.Requestor;
                out_msg.Destination.add(dir_entry.Sharers.smallestElement());
                out_msg.MessageSize := MessageSizeType:Request_Control;
            }
        }
    }
}

action(e_sendDataToGetSRequestors, "e", desc="Send data from cache to all GetS IDs") {
    assert(is_valid(tbe));
    assert(tbe.L1_GetS_IDs.count() > 0);
    enqueue(responseL2Network_out, ResponseMsg, to_l1_latency) {
        DirEntry dir_entry := getDirEntry(address);
        assert(is_valid(dir_entry));
        out_msg.addr := address;
        out_msg.Type := CoherenceResponseType:DATA;
        out_msg.Sender := machineID;
        out_msg.Destination := tbe.L1_GetS_IDs;  // internal nodes
        out_msg.DataBlk := tbe.DataBlk;
        out_msg.MessageSize := MessageSizeType:Response_Data;
        DPRINTF(RubySlicc, "e_sendDataToGetSRequestors::addr=%x, Destination: %s, DataBlock: %s\n"
                         , out_msg.addr, out_msg.Destination, out_msg.DataBlk);
    }
}

action(ex_sendExclusiveDataToGetSRequestors, "ex", desc="Send data from cache to all GetS IDs") {
    assert(is_valid(tbe));
    assert(tbe.L1_GetS_IDs.count() == 1);
    enqueue(responseL2Network_out, ResponseMsg, to_l1_latency) {
        //DirEntry dir_entry := getDirEntry(address);
        //assert(is_valid(dir_entry));
        out_msg.addr := address;
        out_msg.Type := CoherenceResponseType:DATA_EXCLUSIVE;
        out_msg.Sender := machineID;
        out_msg.Destination := tbe.L1_GetS_IDs;  // internal nodes
        out_msg.DataBlk := tbe.DataBlk;
        out_msg.MessageSize := MessageSizeType:Response_Data;
        DPRINTF(RubySlicc, "ex_sendExclusiveDataToGetSRequestors::addr=%x, Destination: %s, DataBlock: %s\n"
                         , out_msg.addr, out_msg.Destination, out_msg.DataBlk);
    }
}

action(ee_sendDataToGetXRequestor, "ee", desc="Send data from cache to GetX ID") {
    enqueue(responseL2Network_out, ResponseMsg, to_l1_latency) {
        assert(is_valid(tbe));
        //DirEntry dir_entry := getDirEntry(address);
        //assert(is_valid(dir_entry));
        out_msg.addr := address;
        out_msg.Type := CoherenceResponseType:DATA;
        out_msg.Sender := machineID;
        out_msg.Destination.add(tbe.L1_GetX_ID);
        out_msg.DataBlk := tbe.DataBlk;
        out_msg.MessageSize := MessageSizeType:Response_Data;
        DPRINTF(RubySlicc, "ee_sendDataToGetXRequestor::addr=%x, Destination: %s, DataBlock: %s\n"
                         , out_msg.addr, out_msg.Destination, out_msg.DataBlk);
    }
}

action(f_sendInvToSharers, "f", desc="invalidate sharers for L2 replacement") {
    enqueue(L1RequestL2Network_out, RequestMsg, to_l1_latency) {
        DirEntry dir_entry := getDirEntry(address);
        assert(is_valid(dir_entry));
        assert(is_valid(tbe));
        DPRINTF(RubySlicc, "f_sendInvToSharers::addr=%x, dest=%s\n"
                         , address, dir_entry.Sharers);
        tbe.dirToEvict  := true;
        out_msg.addr := address;
        out_msg.Type := CoherenceRequestType:INV_EVC;
        out_msg.Requestor := machineID;
        out_msg.Destination := dir_entry.Sharers;
        out_msg.MessageSize := MessageSizeType:Request_Control;
        // counter
        SF.profileEvictions();
        SF.profileSharersNum(dir_entry.Sharers.count());
        L2cache.profileRemainedEntry(address);
    }
}

action(fwm_sendFwdInvToSharersMinusRequestor, "fwm", desc="invalidate sharers for request, requestor is sharer") {
    peek(L1RequestL2Network_in, RequestMsg) {
        enqueue(L1RequestL2Network_out, RequestMsg, to_l1_latency) {
            DirEntry dir_entry := getDirEntry(address);
            //assert(is_valid(cache_entry));
            assert(is_valid(dir_entry));
            out_msg.addr := address;
            out_msg.Type := CoherenceRequestType:INV;
            out_msg.Requestor := in_msg.Requestor;
            out_msg.Destination := dir_entry.Sharers;
            out_msg.Destination.remove(in_msg.Requestor);
            out_msg.MessageSize := MessageSizeType:Request_Control;
            DPRINTF(RubySlicc, "fwm_sendFwdInvToSharersMinusRequestor::addr=%x, sharers=%d\n"
                             , address, out_msg.Destination);
        }
    }
}

// OTHER ACTIONS
action(i_allocateTBE, "i", desc="Allocate TBE for request") {
    if (is_invalid(tbe)) {
        check_allocate(TBEs);
        DirEntry dir_entry := getDirEntry(address);
        State state := getState(tbe, cache_entry, address);
        //assert(is_valid(dir_entry));
        DPRINTF(RubySlicc, "i_allocateTBE::addr=%x, state=%d\n"
                        , address, state);

        TBEs.allocate(address);
        set_tbe(TBEs[address]);
        tbe.L1_GetS_IDs.clear();
        // must set state in advance
        tbe.TBEState := state;
        if (is_valid(cache_entry)) {
            tbe.DataBlk := cache_entry.DataBlk;
            tbe.Dirty := cache_entry.Dirty;
        }
        if (is_valid(dir_entry)) {
            tbe.pendingAcks := dir_entry.Sharers.count();
        }
    } else {
        DPRINTF(RubySlicc, "i_allocateTBE::has existed::addr=%x, state=%d\n"
                         , address, getState(tbe, cache_entry, address));
    }
}

action(s_deallocateTBE, "s", desc="Deallocate external TBE") {
    if (is_valid(tbe)) {
        TBEs.deallocate(address);
        unset_tbe();
    }
}

action(jj_popL1RequestQueue, "\j", desc="Pop incoming L1 request queue") {
    Tick delay := L1RequestL2Network_in.dequeue(clockEdge());
    profileMsgDelay(0, ticksToCycles(delay));
}

action(jjt_popTriggerPort, "\jjt", desc="Pop trigger queue") {
    Tick delay := triggerInPort.dequeue(clockEdge());
    profileMsgDelay(0, ticksToCycles(delay));
}

action(k_popUnblockQueue, "k", desc="Pop incoming unblock queue") {
    Tick delay := L1unblockNetwork_in.dequeue(clockEdge());
    profileMsgDelay(0, ticksToCycles(delay));
}

action(o_popIncomingResponseQueue, "o", desc="Pop Incoming Response queue") {
    Tick delay := responseL2Network_in.dequeue(clockEdge());
    profileMsgDelay(1, ticksToCycles(delay));
}

action(m_allocDataFromMem, "ad", desc="Allocate data from mem to tbe") {
    peek(responseL2Network_in, ResponseMsg) {
        DPRINTF(RubySlicc, "m_allocDataFromMem::addr=%x, datablk=%s\n"
                         , address, in_msg.DataBlk);
        assert(is_valid(tbe));
        tbe.DataBlk := in_msg.DataBlk;
    }
}

action(m_writeDataToCache, "m", desc="Write data from response queue to cache") {
    peek(responseL2Network_in, ResponseMsg) {
        assert(is_valid(cache_entry));
        cache_entry.DataBlk := in_msg.DataBlk;
        if (in_msg.Dirty) {
            cache_entry.Dirty := in_msg.Dirty;
        }
    }
}

action(mr_writeDataToCacheFromRequest, "mr", desc="Write data from request queue to cache") {
    peek(L1RequestL2Network_in, RequestMsg) {
        DPRINTF(RubySlicc, "mr_writeDataToCacheFromRequest::addr=%x, dirty=%d\n"
                         , in_msg.addr, in_msg.Dirty);
        if (in_msg.Dirty) {
            // need to allocate cache
            if (is_invalid(cache_entry)) {
                if (L2cache.cacheAvail(address)) {
                    set_cache_entry(L2cache.allocate(address, new CacheEntry));
                    cache_entry.DataBlk := in_msg.DataBlk;
                    cache_entry.Dirty := in_msg.Dirty;
                    L2cache.removePendingAddr(in_msg.addr);
                } else {
                    State orgState := getState(tbe, cache_entry, in_msg.addr);
                    // need replacement
                    Addr victim := L2cache.cacheProbe(in_msg.addr);
                    CacheEntry L2cache_entry := getCacheEntry(victim);
                    TBE tbe_victim := TBEs[victim];
                    DPRINTF(RubySlicc, "mr_writeDataToCacheFromRequest::Cache victim::addr=%x, state=%d, victim=%x, state=%d\n"
                                     , in_msg.addr, orgState, victim, getState(tbe_victim, L2cache_entry, victim));
                    // set dir_entry flag, to block other request
                    if (orgState == State:MT) {
                        DirEntry dir_entry := getDirEntry(address);
                        assert(is_valid(dir_entry));
                        dir_entry.waitEvict := true;
                    }
                    stall_and_wait(L1RequestL2Network_in, victim);
                    // trigger cache replacement by trigger port
                    enqueue(triggerOutPort, RequestMsg, 0) {
                        out_msg.addr := victim;
                        out_msg.Type := CoherenceRequestType:EVC;
                        out_msg.AccessMode := in_msg.AccessMode;
                        out_msg.MessageSize := in_msg.MessageSize;
                        out_msg.DataBlk := L2cache_entry.DataBlk;
                        out_msg.Dirty := L2cache_entry.Dirty;
                        out_msg.Len := 1;
                    }
                }
            } else {
                cache_entry.DataBlk := in_msg.DataBlk;
                cache_entry.Dirty := in_msg.Dirty;
                L2cache.removePendingAddr(in_msg.addr);
            }
        } else {
            // for clean write back, still need to remove
            // possible pending address
            L2cache.removePendingAddr(in_msg.addr);
        }
        if (is_valid(cache_entry)) {
            // trigger PUTX-end
            DPRINTF(RubySlicc, "mr_writeDataToCacheFromRequest::PUTX end::addr=%x\n"
                             , in_msg.addr);
            enqueue(triggerOutPort, RequestMsg, 0) {
                out_msg.addr := in_msg.addr;
                out_msg.Type := in_msg.Type;
                out_msg.AccessMode := in_msg.AccessMode;
                out_msg.Requestor := in_msg.Requestor;
                out_msg.Destination := in_msg.Destination;
                out_msg.MessageSize := in_msg.MessageSize;
                out_msg.DataBlk := in_msg.DataBlk;
                out_msg.Len := in_msg.Len;
                out_msg.Dirty := in_msg.Dirty;
                out_msg.Prefetch := in_msg.Prefetch;
            }
        }
    }
}

action(mr_writeDataToCacheFromRespond, "mrs", desc="Write data from response queue to cache") {
    peek(responseL2Network_in, ResponseMsg) {
        State state := getState(tbe, cache_entry, address);
        DPRINTF(RubySlicc, "mr_writeDataToCacheFromRespond::addr=%x, dirty=%d, state=%d\n"
                         , in_msg.addr, in_msg.Dirty, state);
        // need to allocate cache
        if (is_invalid(cache_entry)) {
            if (L2cache.cacheAvail(address)) {
                set_cache_entry(L2cache.allocate(address, new CacheEntry));
                cache_entry.DataBlk := in_msg.DataBlk;
                cache_entry.Dirty := in_msg.Dirty;
            } else {
                // it is possible the victim is still in the stall
                // when data back. Hence, if state is IS/MT_IIB and data 
                // may be clean, we do not allocate cache here, since
                // data will be kept in the upstream even if it is dirty
                assert(!in_msg.Dirty || (in_msg.Dirty && (state == State:MT_IB ||
                                                          state == State:MT_IIB)));
                // TODO: MAY BE FAULT!!
            }
        } else {
            DPRINTF(RubySlicc, "mr_writeDataToCacheFromRespond::do not allocate\n");
            cache_entry.DataBlk := in_msg.DataBlk;
            cache_entry.Dirty := in_msg.Dirty;
        }
    }
}

action(c_checkCache, "ccc", desc="Check cache can be allocate before back invalidation") {
    //peek(L1RequestL2Network_in, RequestMsg) {
    //assert(is_valid(tbe));
    bool request_need_allocate_cache := needAllocateCache(getState(tbe, cache_entry, address), address, cache_entry);
    bool evict_need_allocate_cache := isCacheToEvict(tbe) || isDirToEvict(tbe);
    //assert(evict_need_allocate_cache || request_need_allocate_cache);
    DPRINTF(RubySlicc, "c_checkCache::addr=%x, evict_need_ac=%d, request_need_ac=%d\n"
                    , address, evict_need_allocate_cache, request_need_allocate_cache);

    if (!L2cache.cacheAvail(address) && (evict_need_allocate_cache || request_need_allocate_cache)) {
        Addr victim := L2cache.cacheProbe(address);
        CacheEntry L2cache_entry := getCacheEntry(victim);
        TBE tbe_victim := TBEs[victim];
        DPRINTF(RubySlicc, "c_checkCache::Cache victim::addr=%x, victim=%x, victim state=%d\n"
                        , address, victim, getState(tbe_victim, L2cache_entry, victim));

        enqueue(triggerOutPort, RequestMsg, 0) {
            out_msg.addr := victim;
            out_msg.Type := CoherenceRequestType:EVC;
            out_msg.DataBlk := L2cache_entry.DataBlk;
            out_msg.Dirty := L2cache_entry.Dirty;
            if (evict_need_allocate_cache) {
                out_msg.Len := 2;
                if (is_valid(tbe)) {
                    tbe.hasprofileBIDirty := true; // flag to count
                }
            } else {
                out_msg.Len := 3;
            }
        }
        // directly allocate cache here
        // otherwise, the other request
        // will opccupy the cache line
        // save cache line for incoming request
        L2cache_entry.aheadAllocate := true;
        L2cache_entry.aheadAddr := address;
        // set MRU to stop replacement twice
        L2cache.setMRU(victim);

    } else if (evict_need_allocate_cache || request_need_allocate_cache) {
        // still need save pending address
        L2cache.savePendingAddr(address, machineID);
    }
    //}
}

action(q_updateAck, "q", desc="update pending ack count") {
    peek(responseL2Network_in, ResponseMsg) {
        assert(is_valid(tbe));
        assert(in_msg.AckCount < 2);
        tbe.pendingAcks := tbe.pendingAcks - 1;
        APPEND_TRANSITION_COMMENT(in_msg.AckCount);
        APPEND_TRANSITION_COMMENT(" p: ");
        APPEND_TRANSITION_COMMENT(tbe.pendingAcks);
    }
}

action(qq_writeDataToTBE, "\qq", desc="Write data from response queue to TBE") {
    peek(responseL2Network_in, ResponseMsg) {
        assert(is_valid(tbe));
        DPRINTF(RubySlicc, "qq_writeDataToTBE::addr=%x, type=%d, dirty=%d\n"
                         , address, in_msg.Type, in_msg.Dirty);
        if (in_msg.Type == CoherenceResponseType:DATA && in_msg.Dirty) {
            tbe.DataBlk := in_msg.DataBlk;
            tbe.Dirty := in_msg.Dirty;
            tbe.dataBackInvalid := true;
        }
    }
}

action(ss_recordGetSL1ID, "\s", desc="Record L1 GetS for load response") {
    peek(L1RequestL2Network_in, RequestMsg) {
        assert(is_valid(tbe));
        tbe.L1_GetS_IDs.add(in_msg.Requestor);
    }
}

action(xx_recordGetXL1ID, "\x", desc="Record L1 GetX for store response") {
    peek(L1RequestL2Network_in, RequestMsg) {
        assert(is_valid(tbe));
        tbe.L1_GetX_ID := in_msg.Requestor;
        DPRINTF(RubySlicc, "xx_recordGetXL1ID::addr=%x, GetX_ID=%d\n"
                         , address, in_msg.Requestor);
    }
}

action(set_setMRU, "\set", desc="set the MRU entry") {
    L2cache.setMRU(address);
    SF.setMRU(address);
}

action(qq_allocateL2CacheBlock, "\q", desc="Set L2 cache tag equal to tag of block B.") {
    DPRINTF(RubySlicc, "qq_allocateL2CacheBlock::addr=%x\n", address);
    assert(L2cache.cacheAvail(address));
    if (is_invalid(cache_entry)) {
        set_cache_entry(L2cache.allocate(address, new CacheEntry));
    }
}

action(qq_allocateDirBlock, "\qdir", desc="Set dir tag equal to tag of block B.") {
    DPRINTF(RubySlicc, "qq_allocateDirBlock::addr=%x\n", address);
    assert(SF.cacheAvail(address));
    DirEntry dir_entry := getDirEntry(address);
    if (is_invalid(dir_entry)) {
        assert(!SF.isTagPresent(address));
        SF.allocate(address, new DirEntry);
        SF.removePendingAddr(address);
    }
}

action(i_insertDirBk, "\ibk", desc="Insert dir tag to cache as back up.") {
    peek(L1RequestL2Network_in, RequestMsg) {
        DPRINTF(RubySlicc, "i_insertDirBk::addr=%x, Requestor=%d\n"
                         , address, in_msg.Requestor);
        set_cache_entry(L2cache.insertDirBk(address, in_msg.Requestor, new CacheEntry));
    }
}

action(s_switchDirBk, "\sbk", desc="switch dir tag from directory to cache as back up.") {
    DirEntry dir_entry := getDirEntry(address);
    assert(is_valid(dir_entry));
    DPRINTF(RubySlicc, "s_switchDirBk::addr=%x, Requestor=%d, exclusive=%d\n"
                     , address, dir_entry.Sharers, dir_entry.Exclusive);
    assert(dir_entry.Sharers.count() == 1);
    //assert(dir_entry.Exclusive == dir_entry.Sharers.smallestElement());
    set_cache_entry(L2cache.insertDirBk(address, dir_entry.Sharers.smallestElement(), new CacheEntry));
}

action(k_removeDirBk, "\kbk", desc="remove dir tag in cache as back up.") {
    peek(L1RequestL2Network_in, RequestMsg) {
        L2cache.removeDirBk(address);
        unset_cache_entry();
        State state := getState(tbe, cache_entry, address);
        DPRINTF(RubySlicc, "k_removeDirBk::addr=%x, Requestor=%d, cacheline valid=%d, state=%d\n"
                         , address, in_msg.Requestor, is_valid(cache_entry), state);
    }
}

action(qk_allocateDirFromCache, "\qk", desc="allocate directory from cache.") {
    assert(SF.cacheAvail(address));
    assert(is_invalid(getDirEntry(address)));
    assert(!SF.isTagPresent(address));
    SF.allocate(address, new DirEntry);
    SF.removePendingAddr(address);

    DirEntry dir_entry := getDirEntry(address);
    dir_entry.Exclusive := L2cache.getSharerinBackup(address);
    addSharer(address, dir_entry.Exclusive, dir_entry);

    L2cache.removeDirBk(address);
    unset_cache_entry();
    State state := getState(tbe, cache_entry, address);
    DPRINTF(RubySlicc, "qk_allocateDirFromCache::addr=%x, exclusive=%d, cacheline valid=%d, state=%d\n"
                     , address, dir_entry.Exclusive, is_valid(cache_entry), state);
}

action(rr_deallocateL2CacheBlock, "\r", desc="Deallocate L2 cache block.  Sets the cache to not present, allowing a replacement in parallel with a fetch.") {
    bool isCacheEvict := false;
    if(triggerInPort.isReady(clockEdge())) {
        peek(triggerInPort, RequestMsg) {
            isCacheEvict := (in_msg.Type == CoherenceRequestType:EVC);
        }
    }
    assert(is_valid(tbe));
    assert(is_valid(cache_entry));
    // set tbe to be eivct
    // but may encounter BUG
    tbe.cacheToEvict := isCacheEvict;
    bool ahead_allocate := cache_entry.aheadAllocate;
    Addr ahead_addr := cache_entry.aheadAddr;
    DPRINTF(RubySlicc, "rr_deallocateL2CacheBlock::addr=%x, isCacheEvict=%d, allocaahead=%d, aheadAddr=%x\n"
                     , address, isCacheEvict, cache_entry.aheadAllocate, cache_entry.aheadAddr);
    L2cache.deallocate(address);
    unset_cache_entry();
    if (ahead_allocate) {
        L2cache.savePendingAddr(ahead_addr, machineID);
    }
}

action(rr_deallocateDirBlock, "\rdir", desc="Deallocate dir.  Sets the dir to not present, allowing a replacement in parallel with a fetch.") {
    DPRINTF(RubySlicc, "rr_deallocateDirBlock::addr=%x\n"
                     , address);
    assert(SF.isTagPresent(address));
    SF.deallocate(address);
}

action(t_sendWBAck, "t", desc="Send writeback ACK") {
    peek(L1RequestL2Network_in, RequestMsg) {
        enqueue(responseL2Network_out, ResponseMsg, to_l1_latency) {
            out_msg.addr := address;
            out_msg.Type := CoherenceResponseType:WB_ACK;
            out_msg.Sender := machineID;
            out_msg.Destination.add(in_msg.Requestor);
            out_msg.MessageSize := MessageSizeType:Response_Control;
        }
    }
}

action(tt_sendSilentToMem, "ttm", desc="Send ACK_Silent to mem") {
    assert(is_valid(tbe));
    DPRINTF(RubySlicc, "tt_sendSilentToMem::addr=%x, hasSendDataToMem=%d\n"
                     , address, tbe.hasSendDataToMem);
    if(!tbe.hasSendDataToMem) {
        enqueue(responseL2Network_out, ResponseMsg, 1) {
            out_msg.addr := address;
            out_msg.Type := CoherenceResponseType:ACK_Silent;
            // ACK_Silent is only used here
            // to notice memory control that certain block is not maintained
            // by the upstream 
            out_msg.Sender := machineID;
            out_msg.Destination.add(mapAddressToMachine(address, MachineType:Directory));
            out_msg.MessageSize := MessageSizeType:Response_Control;
        }
    }
}

action(t_sendDataToMem, "tm", desc="Send data to mem when cache is unavail") {
    peek(responseL2Network_in, ResponseMsg) {
        assert(in_msg.Dirty);
        assert(is_valid(tbe));
        DPRINTF(RubySlicc, "t_sendDataToMem::addr=%x, hasSendDataToMem=%d, isDirty=%d\n"
                         , address, tbe.hasSendDataToMem, in_msg.Dirty);
        if(!tbe.hasSendDataToMem) {
            tbe.hasSendDataToMem := true;
            enqueue(responseL2Network_out, ResponseMsg, l2_request_latency) {
                out_msg.addr := address;
                out_msg.Type := CoherenceResponseType:MEMORY_DATA;
                out_msg.Sender := machineID;
                out_msg.Destination.add(mapAddressToMachine(address, MachineType:Directory));
                out_msg.DataBlk := in_msg.DataBlk;
                out_msg.Dirty := in_msg.Dirty;
                out_msg.MessageSize := MessageSizeType:Response_Data;
            }
        }
    }
}

action(ts_sendInvAckToUpgrader, "ts", desc="Send ACK to upgrader") {
    peek(L1RequestL2Network_in, RequestMsg) {
        enqueue(responseL2Network_out, ResponseMsg, to_l1_latency) {
            DirEntry dir_entry := getDirEntry(address);
            //assert(is_valid(cache_entry));
            assert(is_valid(dir_entry));
            DPRINTF(RubySlicc, "ts_sendInvAckToUpgrader::addr=%x, requestor=%d, dir_sharers=%d\n"
                             , address, in_msg.Requestor, dir_entry.Sharers);
            assert(dir_entry.Sharers.isElement(in_msg.Requestor));
            out_msg.addr := address;
            out_msg.Type := CoherenceResponseType:ACK;
            out_msg.Sender := machineID;
            out_msg.Destination.add(in_msg.Requestor);
            out_msg.MessageSize := MessageSizeType:Response_Control;
            // upgrader doesn't get ack from itself, hence the + 1
            out_msg.AckCount := 0 - dir_entry.Sharers.count() + 1;
        }
    }
}

action(ts_sendInvAckToUpgraderBk, "tsbk", desc="Send ACK to upgrader when cache line in back up") {
    peek(L1RequestL2Network_in, RequestMsg) {
        enqueue(responseL2Network_out, ResponseMsg, to_l1_latency) {
            DPRINTF(RubySlicc, "ts_sendInvAckToUpgraderBk::addr=%x, requestor=%d\n"
                             , address, in_msg.Requestor);
            assert(L2cache.getSharerinBackup(address) == in_msg.Requestor);
            out_msg.addr := address;
            out_msg.Type := CoherenceResponseType:ACK;
            out_msg.Sender := machineID;
            out_msg.Destination.add(in_msg.Requestor);
            out_msg.MessageSize := MessageSizeType:Response_Control;
        }
    }
}

action(uu_profileMiss, "\um", desc="Profile the demand miss") {
    L2cache.profileDemandMiss();
}

action(uu_profileHit, "\uh", desc="Profile the demand hit") {
    L2cache.profileDemandHit();
}

action(uu_profileBIDirty, "\ubid", desc="Profile the back invalidation but dirty") {
    assert(is_valid(tbe));
    if (tbe.hasprofileBIDirty) {
        L2cache.profileBIDirty();
        tbe.hasprofileBIDirty := false;
    }
}

action(nn_addSharer, "\n", desc="Add L1 sharer to list") {
    peek(L1RequestL2Network_in, RequestMsg) {
        DirEntry dir_entry := getDirEntry(address);
        assert(is_valid(dir_entry));
        addSharer(address, in_msg.Requestor, dir_entry);
        APPEND_TRANSITION_COMMENT( dir_entry.Sharers );
    }
}

action(nnu_addSharerFromUnblock, "\nu", desc="Add L1 sharer to list") {
    peek(L1unblockNetwork_in, ResponseMsg) {
        DirEntry dir_entry := getDirEntry(address);
        DPRINTF(RubySlicc, "nnu_addSharerFromUnblock::addr=%x\n", address);
        //assert(is_valid(cache_entry));
        assert(is_valid(dir_entry));
        addSharer(address, in_msg.Sender, dir_entry);
    }
}

action(kk_removeRequestSharer, "\k", desc="Remove L1 Request sharer from list") {
    peek(L1RequestL2Network_in, RequestMsg) {
        DirEntry dir_entry := getDirEntry(address);
        bool is_sharer := isSharer(address, in_msg.Requestor, dir_entry);
        if (is_sharer) {
            dir_entry.Sharers.remove(in_msg.Requestor);
            // check whether need to remove dir_entry
            if (dir_entry.Sharers.count() == 0) {
                assert(SF.isTagPresent(address));
                // set state before remove
                if (in_msg.Dirty) {
                    dir_entry.DirState := State:M;
                } else {
                    dir_entry.DirState := State:NP;
                    //if (isDirty(cache_entry) && getState(tbe, cache_entry, address) == State:SS) {
                        // if state is SS and dirty, which means LLC do not maintain the
                        // RW permission, we need to silent update directory
                    enqueue(responseL2Network_out, ResponseMsg, 1) {
                        out_msg.addr := address;
                        out_msg.Type := CoherenceResponseType:ACK_Silent;
                        // ACK_Silent is only used here
                        // to notice memory control that certain block is not maintained
                        // by the upstream 
                        out_msg.Sender := machineID;
                        out_msg.Destination.add(mapAddressToMachine(address, MachineType:Directory));
                        out_msg.MessageSize := MessageSizeType:Response_Control;
                    }
                    //}
                }
                SF.deallocate(address);
            }
        }
        // remove wait victim eviction flag
        if (is_valid(dir_entry)) {
            dir_entry.waitEvict := false;
        }
        DPRINTF(RubySlicc, "kk_removeRequestSharer::addr=%x, requestor=%d, is_sharer=%d\n"
                         , address, in_msg.Requestor, is_sharer);
    }
}

action(ll_clearSharers, "\l", desc="Remove all L1 sharers from list") {
    peek(L1RequestL2Network_in, RequestMsg) {
        DirEntry dir_entry := getDirEntry(address);
        assert(is_valid(dir_entry));
        if (dir_entry.DirState == State:MT) {
            assert(dir_entry.Sharers.count() == 1);
        }
        dir_entry.Sharers.clear();
    }
}

action(mmd_markExclusiveFromDir, "\md", desc="set the exclusive owner") {
    DirEntry dir_entry := getDirEntry(address);
    assert(is_valid(dir_entry));
    assert(dir_entry.Sharers.count() == 1);
    //dir_entry.Sharers.clear();
    dir_entry.Exclusive := dir_entry.Sharers.smallestElement();
    //addSharer(address, in_msg.Sender, dir_entry);
    DPRINTF(RubySlicc, "mmd_markExclusiveFromDir::addr=%x, exclusive=%d\n"
                     , address, dir_entry.Exclusive);
}

action(mmu_markExclusiveFromUnblock, "\mu", desc="set the exclusive owner") {
    peek(L1unblockNetwork_in, ResponseMsg) {
        DPRINTF(RubySlicc, "mmu_markExclusiveFromUnblock::addr=%x, exclusive=%d\n"
                         , address, in_msg.Sender);
        DirEntry dir_entry := getDirEntry(address);
        //assert(is_valid(cache_entry));
        assert(is_valid(dir_entry));
        dir_entry.Sharers.clear();
        dir_entry.Exclusive := in_msg.Sender;
        addSharer(address, in_msg.Sender, dir_entry);
    }
}

action(sts_setBkStateISS, "\stsISS", desc="set back up state as ISS") {
    DPRINTF(RubySlicc, "sts_setBkStateISS::addr=%x\n", address);
    L2cache.setDirState(address, "ISS");
}

action(sts_setBkStateIM, "\stsIM", desc="set back up state as IM") {
    DPRINTF(RubySlicc, "sts_setBkStateIM::addr=%x\n", address);
    L2cache.setDirState(address, "IM");
}

action(sts_setBkStateMTMB, "\stsMTMB", desc="set back up state as MT_MB") {
    DPRINTF(RubySlicc, "sts_setBkStateMTTB::addr=%x\n", address);
    L2cache.setDirState(address, "MT_MB");
}

action(sts_setBkStateMT, "\stsMT", desc="set back up state as MT") {
    DPRINTF(RubySlicc, "sts_setBkStateMT::addr=%x\n", address);
    L2cache.setDirState(address, "MT");
}

action(rrf_removeCacheEvictFlag, "\rrf", desc="remove cache evict flag when state is I_I") {
    DPRINTF(RubySlicc, "rrf_removeCacheEvictFlag::addr=%x\n", address);
    assert(is_valid(tbe));
    tbe.cacheToEvict := false;
}

action(rp_removeCachePendingAddr, "rp", desc="remove pending address") {
    L2cache.removePendingAddr(address);
}

action(zz_stallAndWaitL1RequestQueue, "zz", desc="recycle L1 request queue") {
    stall_and_wait(L1RequestL2Network_in, address);
}

action(zzt_stallAndWaitTriggerQueue, "zzt", desc="recycle L1 request queue") {
    stall_and_wait(triggerInPort, address);
}

action(zn_recycleResponseNetwork, "zn", desc="recycle memory request") {
    responseL2Network_in.recycle(clockEdge(), cyclesToTicks(recycle_latency));
}

action(kd_wakeUpDependents, "kd", desc="wake-up dependents") {
    wakeUpBuffers(address);
}

//*****************************************************
// TRANSITIONS
//*****************************************************


//===============================================
// BASE STATE - I

transition({M_I, I_I, S_I, ISS, IS, IM, SS_MB, MT_MB, MT_IIB, MT_IB, MT_SB},  
           {L1_PUTX_front, L1_PUTX_end, L1_PUTX_clean}){
    zz_stallAndWaitL1RequestQueue;
}

transition({MT,M,NP,BK}, Pend){
    zz_stallAndWaitL1RequestQueue;
}

transition({NP,M,M_I,S_I,I_I}, Pop_ReqIn){
}

transition({I_I, S_I, ISS, IS, IM, SS_MB, MT_MB, MT_IIB, MT_IB, MT_SB},  
           {SF_Replacement_withC, SF_Replacement}){
    zzt_stallAndWaitTriggerQueue;
}

transition({NP, SS, M, MT}, L1_PUTX_front) {
    mr_writeDataToCacheFromRequest;
}

transition(BK, L1_PUTX_front_BK, M) {
    k_removeDirBk;
    //TODO: solve problems when back up 
    // directory in cache has more than
    // one sharer
    mr_writeDataToCacheFromRequest;
    // remove directory will debound address
    // and back up cache line. However, cache
    // will allocate L1 victim data and form
    // a new cache line, therefore we need to
    // switch state to M
}

transition({NP, SS, M, MT}, L1_PUTX_end, M) {
    kk_removeRequestSharer;
    // for directory sharers has been clean,
    // we drop directory entry here.
    t_sendWBAck;
    jj_popL1RequestQueue;
    jjt_popTriggerPort;
    kd_wakeUpDependents;
}

transition(SS, L1_PUTX_end_toSS) {
    kk_removeRequestSharer;
    t_sendWBAck;
    jj_popL1RequestQueue;
    jjt_popTriggerPort;
    kd_wakeUpDependents;
}

transition({SS,MT}, L1_PUTX_end_toMT, MT) {
    // add MT, this is the situation that
    // GETX and PUTX happen at the same time
    // GETX will make state to MT, but PUTX
    // also need to switch to MT after remove
    // sharers, so that add MT
    kk_removeRequestSharer;
    mmd_markExclusiveFromDir;
    t_sendWBAck;
    jj_popL1RequestQueue;
    jjt_popTriggerPort;
    kd_wakeUpDependents;
}

transition({NP, SS, M}, L1_PUTX_clean) {
    // ll_clearSharers;
    kk_removeRequestSharer;
    t_sendWBAck;
    jj_popL1RequestQueue;
    kd_wakeUpDependents;
}

transition({SS,MT}, L1_PUTX_clean_toM, M) {
    kk_removeRequestSharer;
    t_sendWBAck;
    jj_popL1RequestQueue;
    kd_wakeUpDependents;
}

transition({SS,MT}, L1_PUTX_clean_pop) {
    kk_removeRequestSharer;
    t_sendWBAck;
    jj_popL1RequestQueue;
    kd_wakeUpDependents;
}

transition({SS,MT}, L1_PUTX_clean_toNP, NP) {
    kk_removeRequestSharer;
    t_sendWBAck;
    jj_popL1RequestQueue;
    kd_wakeUpDependents;
}

transition(BK, L1_PUTX_clean_BK) {
    k_removeDirBk;
    t_sendWBAck;
    jj_popL1RequestQueue;
    kd_wakeUpDependents;
}

transition({SS,M,MT}, L2_Replacement, M_I) {
    i_allocateTBE;
    c_exclusiveReplacement;
    rr_deallocateL2CacheBlock;
    jjt_popTriggerPort;
    //rr_deallocateDirBlock;
}

transition(S_I, L2_Replacement, I_I) {
    // dir eviction encounters cache eviction
    c_exclusiveReplacement;
    rr_deallocateL2CacheBlock;
    jjt_popTriggerPort;
    //rr_deallocateDirBlock;
}

transition({M,MT,SS}, SF_Replacement, I_I) {
    // directory evicts without cache
    i_allocateTBE;
    f_sendInvToSharers;
    rr_deallocateDirBlock;
    //c_checkCache;
    // Once cache is not avail
    // we do not pend current
    // directory eviction
    jjt_popTriggerPort;
}

transition(M_I, SF_Replacement, I_I) {
    // cache eviction but enconter SF
    // evicts the same address
    f_sendInvToSharers;
    rr_deallocateDirBlock;
    //c_checkCache;
    jjt_popTriggerPort;
}

transition({M,MT,SS}, SF_Replacement_withC, S_I) {
    // directory evicts with cache
    i_allocateTBE;
    set_setMRU; 
    // this is to enhance the priority of cache line
    f_sendInvToSharers;
    rr_deallocateDirBlock;
    //c_checkCache;
    jjt_popTriggerPort;
}

transition({MT,M_I,SS}, SF_Switch, BK) {
    // directory switch to cache
    s_switchDirBk;
    sts_setBkStateMT;
    rr_deallocateDirBlock;
    jjt_popTriggerPort;
}

transition({I_I, S_I}, Ack) {
    q_updateAck;
    // collect data to tbe
    qq_writeDataToTBE;
    o_popIncomingResponseQueue;
}

transition(I_I, WB_Data_Keep) {
    q_updateAck;
    qq_writeDataToTBE;
    // may need to send data to memory
    t_sendDataToMem;
    //rp_removeCachePendingAddr;
    o_popIncomingResponseQueue;
}

transition(I_I, Ack_all, NP) {
    tt_sendSilentToMem;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    //rp_removeCachePendingAddr;
    kd_wakeUpDependents;
}

transition(I_I, Ack_all_DtoMem, NP) {
    t_sendDataToMem;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    //rp_removeCachePendingAddr;
    kd_wakeUpDependents;
}

transition(S_I, Ack_all, M) {
    //mr_writeDataToCacheFromRespond;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    //rp_removeCachePendingAddr;
    kd_wakeUpDependents;
}

transition({I_I,S_I}, WB_Data_all, M) {
    mr_writeDataToCacheFromRespond;
    uu_profileBIDirty;
    //rp_removeCachePendingAddr;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
}

transition({IM, IS, ISS, SS_MB, MT_MB, MT_IIB, MT_IB, MT_SB}, {L2_Replacement}) {
    //zz_stallAndWaitL1RequestQueue;
    zzt_stallAndWaitTriggerQueue;
}

transition({IM, IS, ISS, SS_MB, MT_MB, MT_IIB, MT_IB, MT_SB}, MEM_Inv) {
    zn_recycleResponseNetwork;
}

transition({I_I, S_I, M_I, NP}, MEM_Inv) {
    o_popIncomingResponseQueue;
}

transition({SS_MB, MT_MB, MT_IIB, MT_IB, MT_SB}, {L1_GETX, L1_UPGRADE, L1_GETS, L1_GET_INSTR,
                                                  L1_GETX_BK, L1_GETS_BK, L1_GET_INSTR_BK}) {
    zz_stallAndWaitL1RequestQueue;
}

transition(NP, {L1_GETS, L1_GET_INSTR},  ISS) {
    //qq_allocateL2CacheBlock;
    qq_allocateDirBlock;
    ll_clearSharers;
    nn_addSharer;
    i_allocateTBE;
    ss_recordGetSL1ID;
    a_issueFetchToMemory;
    uu_profileMiss;
    jj_popL1RequestQueue;
}

transition(NP, {L1_GETS_BK, L1_GET_INSTR_BK}, BK) {
    i_insertDirBk;
    i_allocateTBE;
    ss_recordGetSL1ID;
    sts_setBkStateISS;
    a_issueFetchToMemory;
    uu_profileMiss;
    jj_popL1RequestQueue;
}

transition(NP, L1_GETX, IM) {
    //qq_allocateL2CacheBlock;
    qq_allocateDirBlock;
    ll_clearSharers;
    // nn_addSharer;
    i_allocateTBE;
    xx_recordGetXL1ID;
    a_issueFetchToMemory;
    uu_profileMiss;
    jj_popL1RequestQueue;
}

transition(NP, L1_GETX_BK, BK) {
    i_insertDirBk;
    i_allocateTBE;
    xx_recordGetXL1ID;
    sts_setBkStateIM;
    a_issueFetchToMemory;
    uu_profileMiss;
    jj_popL1RequestQueue;
}

// transitions from IS/IM
transition(ISS, Mem_Data, MT_MB) {
    m_allocDataFromMem;
    ex_sendExclusiveDataToGetSRequestors;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
}

// transitions from IS/IM
transition(BK, Mem_Data_ISS) {
    // directory is in cache line as back up
    m_allocDataFromMem;
    ex_sendExclusiveDataToGetSRequestors;
    sts_setBkStateMTMB;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
}

transition(IS, Mem_Data, SS) {
    m_allocDataFromMem;
    // allocate data into cache before
    // enter SS state
    mr_writeDataToCacheFromRespond;
    rp_removeCachePendingAddr;
    e_sendDataToGetSRequestors;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
}

transition(IM, Mem_Data, MT_MB) {
    m_allocDataFromMem;
    ee_sendDataToGetXRequestor;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
}

transition(BK, Mem_Data_IM) {
    // directory is in cache line as back up
    m_allocDataFromMem;
    ee_sendDataToGetXRequestor;
    sts_setBkStateMTMB;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
}

transition({IS, ISS}, {L1_GETS, L1_GET_INSTR}, IS) {
    // TODO: need to make sure
    // cache is avail
    c_checkCache;
    nn_addSharer;
    ss_recordGetSL1ID;
    uu_profileMiss;
    jj_popL1RequestQueue;
}

transition({IS, ISS}, L1_GETX) {
    zz_stallAndWaitL1RequestQueue;
}

transition(IM, {L1_GETX, L1_GETS, L1_GET_INSTR}) {
    zz_stallAndWaitL1RequestQueue;
}

// transitions from SS
transition(SS, {L1_GETS, L1_GET_INSTR}, MT_IIB) {
    i_allocateTBE;
    ds_sendSharedDataToRequestor;
    nn_addSharer;
    c_checkCache;
    set_setMRU;
    uu_profileHit;
    jj_popL1RequestQueue;
}

transition(SS, L1_GETS_withC) {
    i_allocateTBE;
    ds_sendSharedDataToRequestor;
    nn_addSharer;
    set_setMRU;
    uu_profileHit;
    jj_popL1RequestQueue;
}

transition(SS, L1_GETX, SS_MB) {
    d_sendDataToRequestor;
    fwm_sendFwdInvToSharersMinusRequestor;
    set_setMRU;
    uu_profileHit;
    jj_popL1RequestQueue;
}

transition({SS,M,MT}, L1_UPGRADE, SS_MB) {
    fwm_sendFwdInvToSharersMinusRequestor;
    ts_sendInvAckToUpgrader;
    set_setMRU;
    uu_profileHit;
    jj_popL1RequestQueue;
}

transition(BK, L1_UPGRADE_BK) {
    ts_sendInvAckToUpgraderBk;
    sts_setBkStateMT;
    set_setMRU;
    uu_profileHit;
    jj_popL1RequestQueue;
}

transition(M, L1_GETX, MT_MB) {
    d_sendDataToRequestor;
    set_setMRU;
    uu_profileHit;
    jj_popL1RequestQueue;
}

transition(M, L1_GET_INSTR, SS) {
    d_sendDataToRequestor;
    nn_addSharer;
    set_setMRU;
    uu_profileHit;
    jj_popL1RequestQueue;
}

transition(M, L1_GETS, MT_MB) {
    dd_sendExclusiveDataToRequestor;
    set_setMRU;
    uu_profileHit;
    jj_popL1RequestQueue;
}

transition(M, {L1_GETS_BK, L1_GETX_BK, L1_UPGRADE_BK}, BK) {
    dbk_sendDataToRequestorforBk;
    //rr_deallocateL2CacheBlock;
    i_insertDirBk;
    sts_setBkStateMTMB;
    set_setMRU;
    uu_profileHit;
    jj_popL1RequestQueue;
}

transition({SS,M,MT},  MEM_Inv, M_I) {
    i_allocateTBE;
    c_exclusiveReplacement;
    rr_deallocateL2CacheBlock;
    //rr_deallocateDirBlock;
}

// transitions from MT
transition(MT, L1_GETX, MT_MB) {
    b_forwardRequestToExclusive;
    uu_profileMiss;
    set_setMRU;
    jj_popL1RequestQueue;
}

transition(MT, {L1_GETS, L1_GET_INSTR}, MT_IIB) {
    b_forwardRequestToExclusive;
    c_checkCache; // need to check cache allovcation
    uu_profileMiss;
    set_setMRU;
    jj_popL1RequestQueue;
}

transition(BK, L1_GETX_BK, MT_MB) {
    qk_allocateDirFromCache;
    b_forwardRequestToExclusive;
    uu_profileMiss;
    set_setMRU;
    jj_popL1RequestQueue;
}

transition(BK, {L1_GETS_BK, L1_GET_INSTR_BK}, MT_IIB) {
    qk_allocateDirFromCache;
    b_forwardRequestToExclusive;
    c_checkCache; // need to check cache allovcation
    uu_profileMiss;
    set_setMRU;
    jj_popL1RequestQueue;
}

transition({SS_MB,MT_MB,MT_IIB}, Exclusive_Unblock, MT) {
    // update actual directory
    mmu_markExclusiveFromUnblock;
    k_popUnblockQueue;
    kd_wakeUpDependents;
}

transition(BK, Exclusive_Unblock) {
    // update directory in cache line
    sts_setBkStateMT;
    k_popUnblockQueue;
    kd_wakeUpDependents;
}

transition({MT_IIB,SS}, Unblock, MT_IB) {
    nnu_addSharerFromUnblock;
    k_popUnblockQueue;
}

transition({MT_IIB,SS,MT_SB}, {WB_Data, WB_Data_clean}, MT_SB) {
    // add MT_SB in the state is for the
    // situation that two requestor acccess
    // at the same time, so there are two
    // unblock can be received
    mr_writeDataToCacheFromRespond;
    rp_removeCachePendingAddr;
    o_popIncomingResponseQueue;
}

transition(MT_IB, {WB_Data, WB_Data_clean}, SS) {
    //m_writeDataToCache;
    mr_writeDataToCacheFromRespond;
    rp_removeCachePendingAddr;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
}

transition(MT_SB, Unblock, SS) {
    nnu_addSharerFromUnblock;
    s_deallocateTBE;
    k_popUnblockQueue;
    kd_wakeUpDependents;
}

transition(MT_SB, Unblock_Keep) {
    nnu_addSharerFromUnblock;
    k_popUnblockQueue;
}

// writeback states
transition({I_I, S_I, M_I}, {L1_GETX, L1_UPGRADE, L1_GETS, L1_GET_INSTR,
                             L1_GETX_BK, L1_GETS_BK, L1_GET_INSTR_BK, L1_UPGRADE_BK}) {
    zz_stallAndWaitL1RequestQueue;
}

transition(M_I, Mem_Ack, NP) {
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
}

transition(BK, Mem_Ack) {
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
}

transition(BK, Mem_Ack_No_deallocTBE) {
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
}

transition({S_I,NP,M,MT,MT_MB,ISS,IM,IS,SS}, Mem_Ack) {
    // this is for cache eviction but
    // encouters SF eviction
    // SS is that tbe has been deallocate,
    // so that pop queue directly
    o_popIncomingResponseQueue;
}

transition(I_I, Mem_Ack) {
    // this is for cache eviction but
    // encouters SF eviction. I_I state
    // need remove cacheEvict flag
    rrf_removeCacheEvictFlag;
    o_popIncomingResponseQueue;
}

transition(M_I, Mem_Ack_MT, MT) {
    mmd_markExclusiveFromDir;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
}

transition(M_I, Mem_Ack_SS, SS) {
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
}